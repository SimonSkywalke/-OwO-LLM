# 基于 **Transformer** 的机器翻译模型

## 嵌入层

### 词嵌入层

```python
# 基本的嵌入层
class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)
```

#### 详细解释

- **功能**：将输入的词（token）索引转换为密集向量表示
- **参数**：
  - `vocab_size`：词汇表大小，即可能的token总数
  - `d_model`：模型的维度，每个token将被映射到d_model维的向量空间
- **工作流程**：
  1. 输入x是形状为`[batch_size, seq_length]`的整数张量，每个整数代表词汇表中的一个token索引
  2. `nn.Embedding`将每个索引替换为对应的d_model维向量
  3. 输出的形状为`[batch_size, seq_length, d_model]`
- **关键点**：
  * **查找表操作**：嵌入本质上是一个查找表操作，根据token ID查找对应的向量表示
  * **可学习参数**：嵌入权重是模型训练过程中通过反向传播学习的参数，不是预设的
  * **向量空间意义**：训练结束后，语义相似的token在嵌入空间中距离更近，反映语义关系
  * **独热编码对比**：相比于独热编码(每个token使用一个位置为1的稀疏向量)，嵌入表示是稠密向量，包含更丰富的语义信息
  * **共享特征**：通过训练，模型可以学习到token间的共享特征，这种特征在嵌入向量的不同维度上表达

嵌入层将Transformer模型的符号输入转换为连续向量空间中的表示，为后续的位置编码层和自注意力机制层提供了基础的特征表示。

#### 简单举例

##### 1. 嵌入矩阵初始化

**嵌入矩阵 E**:

- 形状: `[vocab_size, d_model]`
- 初始化: 通常使用随机初始化，值在 -1 到 1 之间

假设 `vocab_size = 5` 和 `d_model = 3`，初始化的嵌入矩阵可能如下:

```
E = [
    [ 0.3,  0.2, -0.1],  # token_id = 0
    [-0.4,  0.5,  0.9],  # token_id = 1
    [ 0.1, -0.3,  0.7],  # token_id = 2
    [-0.2,  0.8, -0.5],  # token_id = 3
    [ 0.6, -0.1,  0.4]   # token_id = 4
]
```

##### 2. 输入处理与查找操作

输入是包含token ID的整数张量，形状为 `[batch_size, seq_length]`。

```
X = [
    [1, 3, 0],  # 第一个句子
    [4, 2, 3]   # 第二个句子
]
```

形状: `[2, 3]` (batch_size=2, seq_length=3)

##### 3. 嵌入查找操作

对于输入X中的每个token ID，查找嵌入矩阵E中对应行的向量:

**计算过程**:
- 对于 `X[0,0] = 1`: 查找 `E[1] = [-0.4, 0.5, 0.9]`
- 对于 `X[0,1] = 3`: 查找 `E[3] = [-0.2, 0.8, -0.5]`
- 对于 `X[0,2] = 0`: 查找 `E[0] = [0.3, 0.2, -0.1]` 
- 以此类推...

**详细步骤实现**:

1. 初始化输出张量 `Y` 形状为 `[batch_size, seq_length, d_model]` = `[2, 3, 3]`
2. 对于输入X中的每个位置 (i,j):
   - 获取token ID: `token_id = X[i,j]`
   - 查找对应的嵌入向量: `Y[i,j] = E[token_id]`

##### 4. 结果组装

将所有查找结果组装成最终输出张量:

```
Y = [
    [
        [-0.4,  0.5,  0.9],  # X[0,0]=1 对应的向量
        [-0.2,  0.8, -0.5],  # X[0,1]=3 对应的向量
        [ 0.3,  0.2, -0.1]   # X[0,2]=0 对应的向量
    ],
    [
        [ 0.6, -0.1,  0.4],  # X[1,0]=4 对应的向量
        [ 0.1, -0.3,  0.7],  # X[1,1]=2 对应的向量
        [-0.2,  0.8, -0.5]   # X[1,2]=3 对应的向量
    ]
]
```

输出形状: `[2, 3, 3]` (batch_size=2, seq_length=3, d_model=3)



###  位置编码层

```python
class PositionalEncoder(nn.Module):
    # d_model：嵌入向量的维度（模型维度）
    # max_seq_len：最大序列长度，默认为80
    # dropout：随机丢弃率，用于防止过拟合
    def __init__(self, d_model, max_seq_len=80, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(dropout)
        # 根据pos和i创建一个形状为(max_seq_len, d_model)常量PE矩阵 pos：位置索引 i维度索引
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))
                if i + 1 < d_model:
                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))
        # 增加一个批处理维度，将形状从 [max_seq_len, d_model] 变为 [1, max_seq_len, d_model]
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # 使得单词嵌入表示相对大一些
        x = x * math.sqrt(self.d_model)
        # 增加位置常量到单词嵌入表示中
        seq_len = x.size(1)
        x = x + Variable(self.pe[:, :seq_len, :], requires_grad=False)
        return self.dropout(x)
```

- **功能**：为每个token添加表示其在序列中位置的信息
- **为什么需要位置编码**：
  - Transformer模型是并行处理序列的，没有RNN的顺序信息
  - 位置编码使模型能够了解token之间的相对或绝对位置关系

#### 数学原理

位置编码（Positional Encoding）的核心思想是为序列中的每个位置创建唯一的向量表示，使Transformer模型能够感知元素的相对或绝对位置。Transformer论文中采用的是基于正弦和余弦函数的编码方案。

对于位置pos和维度 i，位置编码的数学定义为：

对于偶数维度 2i:
$$
PE_{(pos,i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
对于奇数维度 2i+1:
$$
PE_{(pos,i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
其中：
- $pos$ 是序列中的位置索引（从0开始）

- $i$ 是维度索引（从0开始）

- $d_{model}$ 是模型的嵌入维度

这个公式的关键部分是 $\frac{pos}{10000^{i/d_{model}}}$，它包含两个变量：$pos$ 和 $i$。

分母项为 $10000^{2i/d_{model}}$：
- 在低维度（i比较小时），$2i/d_{model}$ 较小，分母接近 1，输入值 $\frac{pos}{\text{分母}}$ 变化较快，频率较高。这意味着函数在短距离内波动剧烈，适合捕捉序列中**局部位置**的特征。
- 在高维度（i比较大时），$2i/d_{model}$ 较大，分母迅速增大，输入值 $\frac{pos}{\text{分母}}$​ 变化较慢，频率较低。这意味着函数在序列中变化缓慢。适合捕捉序列中**长距离位置**的关系。

为什么选择正弦和余弦函数

1. **唯一性**：不同位置的编码是唯一的，避免了位置冲突。

2. **相对位置可学习**：正弦和余弦函数有一个重要性质 —— 任意位置的编码可以表示为其他位置编码的线性函数。

   对于相对位置 $k$，有以下等式（三角恒等式）：
   $$\sin(\alpha + \beta) = \sin(\alpha)\cos(\beta) + \cos(\alpha)\sin(\beta)$$
   $$\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)$$

   这意味着 $PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性变换，这使得模型更容易学习相对位置关系。

3. **周期性**：正弦和余弦函数的周期性使得编码可以扩展到未见过的序列长度。

#### 简单举例

继续使用上文中的例子

##### 5. 位置编码计算

假设嵌入矩阵如下：
```plaintext
E = [
    [ 0.3,  0.2, -0.1,  0.5],  # token_id = 0
    [-0.4,  0.5,  0.9, -0.7],  # token_id = 1
    [ 0.1, -0.3,  0.7,  0.2],  # token_id = 2
    [-0.2,  0.8, -0.5,  0.3],  # token_id = 3
    [ 0.6, -0.1,  0.4, -0.2]   # token_id = 4
]
```

对于句子 `"我 爱 学习"`：
- 假设 `"我"` 的token_id为1，`"爱"` 的token_id为3，`"学习"` 的token_id为0。
- 嵌入结果：
  ```
  "我" -> [-0.4,  0.5,  0.9, -0.7]
  "爱" -> [-0.2,  0.8, -0.5,  0.3]
  "学习" -> [ 0.3,  0.2, -0.1,  0.5]
  ```

此时：
- **维度索引(i)**表示向量中的每个数值的编号：
  - 对于"我"：维度0是`-0.4`，维度1是`0.5`，维度2是`0.9`，维度3是`-0.7`。

位置索引用于表示序列中每个token的顺序位置。  
- 例如，在句子 `"我 爱 学习"` 中：
  - `"我"` 的位置索引是 `0`
  - `"爱"` 的位置索引是 `1`
  - `"学习"` 的位置索引是 `2`

假设 `d_model = 4`，计算位置索引为0、1、2的编码。

###### **分母计算**
分母为 $10000^{2i/d_{model}}$，具体计算如下：
- 维度0（$2i=0$）：$10000^{0/4} = 1$
- 维度1（$2i+1=1$）：$10000^{1/4} \approx 17.78$
- 维度2（$2i=2$）：$10000^{2/4} = 100$
- 维度3（$2i+1=3$）：$10000^{3/4} \approx 5623.41$

###### 编码计算

计算位置索引为0的编码

- 维度0：$\sin(0/1) = \sin(0) = 0$
- 维度1：$\cos(0/17.78) = \cos(0) = 1$
- 维度2：$\sin(0/100) = \sin(0) = 0$
- 维度3：$\cos(0/5623.41) = \cos(0) = 1$

编码向量：[0, 1, 0, 1]

计算位置索引为1的编码

- 维度0：$\sin(1/1) = \sin(1) \approx 0.8415$
- 维度1：$\cos(1/17.78) \approx 0.9984$
- 维度2：$\sin(1/100) = \sin(0.01) \approx 0.01$
- 维度3：$\cos(1/5623.41) \approx 1$

编码向量：[0.8415, 0.9984, 0.01, 1]

---

##### 6. 嵌入与位置编码的结合

根据 Transformer 的设计，输入到模型中的向量是 **嵌入向量与位置编码向量逐元素相加的结果**，但需要注意的是，在结合之前，**嵌入向量需要先乘以 $\sqrt{d_{model}}$**，以平衡嵌入向量和位置编码的数值范围。

公式如下：
$$\text{最终表示} = (\text{嵌入向量} \cdot \sqrt{d_{model}}) + \text{位置编码}$$

假设通过嵌入层得到的初始嵌入向量为：
```plaintext
"我" -> [-0.4,  0.5,  0.9, -0.7]
"爱" -> [-0.2,  0.8, -0.5,  0.3]
"学习" -> [ 0.3,  0.2, -0.1,  0.5]
```

假设位置编码（通过正弦和余弦函数生成）为：
```plaintext
位置0 -> [0, 1, 0, 1]
位置1 -> [0.8415, 0.9984, 0.01, 1]
位置2 -> [0.9093, 0.9937, 0.02, 1]
```

根据公式，嵌入向量需要乘以 $\sqrt{d_{model}}$：
- 计算 $\sqrt{d_{model}} = \sqrt{4} = 2$
- 缩放后的嵌入向量：
  ```plaintext
  "我" -> [-0.4 * 2,  0.5 * 2,  0.9 * 2, -0.7 * 2] = [-0.8,  1.0,  1.8, -1.4]
  "爱" -> [-0.2 * 2,  0.8 * 2, -0.5 * 2,  0.3 * 2] = [-0.4,  1.6, -1.0,  0.6]
  "学习" -> [0.3 * 2,  0.2 * 2, -0.1 * 2,  0.5 * 2] = [0.6,  0.4, -0.2,  1.0]
  ```

将缩放后的嵌入向量与位置编码逐元素相加：
- `"我"` 的最终表示：
  
  ```plaintext
  [-0.8 + 0,  1.0 + 1,  1.8 + 0, -1.4 + 1] = [-0.8,  2.0,  1.8, -0.4]
  ```
- `"爱"` 的最终表示：
  ```plaintext
  [-0.4 + 0.8415,  1.6 + 0.9984, -1.0 + 0.01,  0.6 + 1] = [0.4415,  2.5984, -0.99,  1.6]
  ```
- `"学习"` 的最终表示：
  ```plaintext
  [0.6 + 0.9093,  0.4 + 0.9937, -0.2 + 0.02,  1.0 + 1] = [1.5093,  1.3937, -0.18,  2.0]
  ```

结合后的向量表示为：
```plaintext
"我" -> [-0.8,  2.0,  1.8, -0.4]
"爱" -> [0.4415,  2.5984, -0.99,  1.6]
"学习" -> [1.5093,  1.3937, -0.18,  2.0]
```













