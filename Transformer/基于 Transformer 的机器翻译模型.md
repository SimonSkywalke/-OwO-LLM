# 基于 **Transformer** 的机器翻译模型

## 嵌入层

### 词嵌入层

```python
# 基本的嵌入层
class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)
```

#### 详细解释

- **功能**：将输入的词（token）索引转换为密集向量表示
- **参数**：
  - `vocab_size`：词汇表大小，即可能的token总数
  - `d_model`：模型的维度，每个token将被映射到d_model维的向量空间
- **工作流程**：
  1. 输入x是形状为`[batch_size, seq_length]`的整数张量，每个整数代表词汇表中的一个token索引
  2. `nn.Embedding`将每个索引替换为对应的d_model维向量
  3. 输出的形状为`[batch_size, seq_length, d_model]`
- **关键点**：
  * **查找表操作**：嵌入本质上是一个查找表操作，根据token ID查找对应的向量表示
  * **可学习参数**：嵌入权重是模型训练过程中通过反向传播学习的参数，不是预设的
  * **向量空间意义**：训练结束后，语义相似的token在嵌入空间中距离更近，反映语义关系
  * **独热编码对比**：相比于独热编码(每个token使用一个位置为1的稀疏向量)，嵌入表示是稠密向量，包含更丰富的语义信息
  * **共享特征**：通过训练，模型可以学习到token间的共享特征，这种特征在嵌入向量的不同维度上表达

嵌入层将Transformer模型的符号输入转换为连续向量空间中的表示，为后续的位置编码层和自注意力机制层提供了基础的特征表示。

#### 简单举例

##### 1. 嵌入矩阵初始化

**嵌入矩阵 E**:

- 形状: `[vocab_size, d_model]`
- 初始化: 通常使用随机初始化，值在 -1 到 1 之间

假设 `vocab_size = 5` 和 `d_model = 3`，初始化的嵌入矩阵可能如下:

```
E = [
    [ 0.3,  0.2, -0.1],  # token_id = 0
    [-0.4,  0.5,  0.9],  # token_id = 1
    [ 0.1, -0.3,  0.7],  # token_id = 2
    [-0.2,  0.8, -0.5],  # token_id = 3
    [ 0.6, -0.1,  0.4]   # token_id = 4
]
```

##### 2. 输入处理与查找操作

输入是包含token ID的整数张量，形状为 `[batch_size, seq_length]`。

```
X = [
    [1, 3, 0],  # 第一个句子
    [4, 2, 3]   # 第二个句子
]
```

形状: `[2, 3]` (batch_size=2, seq_length=3)

##### 3. 嵌入查找操作

对于输入X中的每个token ID，查找嵌入矩阵E中对应行的向量:

**计算过程**:
- 对于 `X[0,0] = 1`: 查找 `E[1] = [-0.4, 0.5, 0.9]`
- 对于 `X[0,1] = 3`: 查找 `E[3] = [-0.2, 0.8, -0.5]`
- 对于 `X[0,2] = 0`: 查找 `E[0] = [0.3, 0.2, -0.1]` 
- 以此类推...

**详细步骤实现**:

1. 初始化输出张量 `Y` 形状为 `[batch_size, seq_length, d_model]` = `[2, 3, 3]`
2. 对于输入X中的每个位置 (i,j):
   - 获取token ID: `token_id = X[i,j]`
   - 查找对应的嵌入向量: `Y[i,j] = E[token_id]`

##### 4. 结果组装

将所有查找结果组装成最终输出张量:

```
Y = [
    [
        [-0.4,  0.5,  0.9],  # X[0,0]=1 对应的向量
        [-0.2,  0.8, -0.5],  # X[0,1]=3 对应的向量
        [ 0.3,  0.2, -0.1]   # X[0,2]=0 对应的向量
    ],
    [
        [ 0.6, -0.1,  0.4],  # X[1,0]=4 对应的向量
        [ 0.1, -0.3,  0.7],  # X[1,1]=2 对应的向量
        [-0.2,  0.8, -0.5]   # X[1,2]=3 对应的向量
    ]
]
```

输出形状: `[2, 3, 3]` (batch_size=2, seq_length=3, d_model=3)



###  位置编码层

```python
class PositionalEncoder(nn.Module):
    # d_model：嵌入向量的维度（模型维度）
    # max_seq_len：最大序列长度，默认为80
    # dropout：随机丢弃率，用于防止过拟合
    def __init__(self, d_model, max_seq_len=80, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(dropout)
        # 根据pos和i创建一个形状为(max_seq_len, d_model)常量PE矩阵 pos：位置索引 i维度索引
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))
                if i + 1 < d_model:
                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))
        # 增加一个批处理维度，将形状从 [max_seq_len, d_model] 变为 [1, max_seq_len, d_model]
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # 使得单词嵌入表示相对大一些
        x = x * math.sqrt(self.d_model)
        # 增加位置常量到单词嵌入表示中
        seq_len = x.size(1)
        x = x + Variable(self.pe[:, :seq_len, :], requires_grad=False)
        return self.dropout(x)
```

- **功能**：为每个token添加表示其在序列中位置的信息
- **为什么需要位置编码**：
  - Transformer模型是并行处理序列的，没有RNN的顺序信息
  - 位置编码使模型能够了解token之间的相对或绝对位置关系

#### 数学原理

位置编码（Positional Encoding）的核心思想是为序列中的每个位置创建唯一的向量表示，使Transformer模型能够感知元素的相对或绝对位置。Transformer论文中采用的是基于正弦和余弦函数的编码方案。

对于位置pos和维度 i，位置编码的数学定义为：

对于偶数维度 2i:
$$
PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
对于奇数维度 2i+1:
$$
PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
其中：
- $pos$ 是序列中的位置索引（从0开始）

- $i$ 是维度索引（从0开始）

- $d_{model}$ 是模型的嵌入维度

这个公式的关键部分是 $\frac{pos}{10000^{i/d_{model}}}$，它包含两个变量：$pos$ 和 $i$。

分母项为 $10000^{2i/d_{model}}$：
- 在低维度（i比较小时），$2i/d_{model}$ 较小，分母接近 1，输入值 $\frac{pos}{\text{分母}}$ 变化较快，频率较高。这意味着函数在短距离内波动剧烈，适合捕捉序列中**局部位置**的特征。
- 在高维度（i比较大时），$2i/d_{model}$ 较大，分母迅速增大，输入值 $\frac{pos}{\text{分母}}$​ 变化较慢，频率较低。这意味着函数在序列中变化缓慢。适合捕捉序列中**长距离位置**的关系。

为什么选择正弦和余弦函数

1. **唯一性**：不同位置的编码是唯一的，避免了位置冲突。

2. **相对位置可学习**：正弦和余弦函数有一个重要性质 —— 任意位置的编码可以表示为其他位置编码的线性函数。

   对于相对位置 $k$，有以下等式（三角恒等式）：
   $$\sin(\alpha + \beta) = \sin(\alpha)\cos(\beta) + \cos(\alpha)\sin(\beta)$$
   $$\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)$$

   这意味着 $PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性变换，这使得模型更容易学习相对位置关系。

3. **周期性**：正弦和余弦函数的周期性使得编码可以扩展到未见过的序列长度。

#### 简单举例

继续使用上文中的例子

##### 5. 位置编码计算

假设嵌入矩阵如下：
```plaintext
E = [
    [ 0.3,  0.2, -0.1,  0.5],  # token_id = 0
    [-0.4,  0.5,  0.9, -0.7],  # token_id = 1
    [ 0.1, -0.3,  0.7,  0.2],  # token_id = 2
    [-0.2,  0.8, -0.5,  0.3],  # token_id = 3
    [ 0.6, -0.1,  0.4, -0.2]   # token_id = 4
]
```

对于句子 `"我 爱 学习"`：
- 假设 `"我"` 的token_id为1，`"爱"` 的token_id为3，`"学习"` 的token_id为0。
- 嵌入结果：
  ```
  "我" -> [-0.4,  0.5,  0.9, -0.7]
  "爱" -> [-0.2,  0.8, -0.5,  0.3]
  "学习" -> [ 0.3,  0.2, -0.1,  0.5]
  ```

此时：
- **维度索引(i)**表示向量中的每个数值的编号：
  - 对于"我"：维度0是`-0.4`，维度1是`0.5`，维度2是`0.9`，维度3是`-0.7`。

位置索引用于表示序列中每个token的顺序位置。  
- 例如，在句子 `"我 爱 学习"` 中：
  - `"我"` 的位置索引是 `0`
  - `"爱"` 的位置索引是 `1`
  - `"学习"` 的位置索引是 `2`

假设 `d_model = 4`，计算位置索引为0、1、2的编码。

分母为 $10000^{2i/d_{model}}$，具体计算如下：
- 维度0（$2i=0$）：$10000^{0/4} = 1$
- 维度1（$2i+1=1$）：$10000^{1/4} \approx 17.78$
- 维度2（$2i=2$）：$10000^{2/4} = 100$
- 维度3（$2i+1=3$）：$10000^{3/4} \approx 5623.41$

计算位置索引为0的编码

- 维度0：$\sin(0/1) = \sin(0) = 0$
- 维度1：$\cos(0/17.78) = \cos(0) = 1$
- 维度2：$\sin(0/100) = \sin(0) = 0$
- 维度3：$\cos(0/5623.41) = \cos(0) = 1$

编码向量：[0, 1, 0, 1]

计算位置索引为1的编码

- 维度0：$\sin(1/1) = \sin(1) \approx 0.8415$
- 维度1：$\cos(1/17.78) \approx 0.9984$
- 维度2：$\sin(1/100) = \sin(0.01) \approx 0.01$
- 维度3：$\cos(1/5623.41) \approx 1$

编码向量：[0.8415, 0.9984, 0.01, 1]

---

##### 6. 嵌入与位置编码的结合

根据 Transformer 的设计，输入到模型中的向量是 **嵌入向量与位置编码向量逐元素相加的结果**，但需要注意的是，在结合之前，**嵌入向量需要先乘以 $\sqrt{d_{model}}$**，以平衡嵌入向量和位置编码的数值范围。

公式如下：
$$\text{最终表示} = (\text{嵌入向量} \cdot \sqrt{d_{model}}) + \text{位置编码}$$

假设通过嵌入层得到的初始嵌入向量为：
```plaintext
"我" -> [-0.4,  0.5,  0.9, -0.7]
"爱" -> [-0.2,  0.8, -0.5,  0.3]
"学习" -> [ 0.3,  0.2, -0.1,  0.5]
```

假设位置编码（通过正弦和余弦函数生成）为：
```plaintext
位置0 -> [0, 1, 0, 1]
位置1 -> [0.8415, 0.9984, 0.01, 1]
位置2 -> [0.9093, 0.9937, 0.02, 1]
```

根据公式，嵌入向量需要乘以 $\sqrt{d_{model}}$：
- 计算 $\sqrt{d_{model}} = \sqrt{4} = 2$
- 缩放后的嵌入向量：
  ```plaintext
  "我" -> [-0.4 * 2,  0.5 * 2,  0.9 * 2, -0.7 * 2] = [-0.8,  1.0,  1.8, -1.4]
  "爱" -> [-0.2 * 2,  0.8 * 2, -0.5 * 2,  0.3 * 2] = [-0.4,  1.6, -1.0,  0.6]
  "学习" -> [0.3 * 2,  0.2 * 2, -0.1 * 2,  0.5 * 2] = [0.6,  0.4, -0.2,  1.0]
  ```

将缩放后的嵌入向量与位置编码逐元素相加：
- `"我"` 的最终表示：
  
  ```plaintext
  [-0.8 + 0,  1.0 + 1,  1.8 + 0, -1.4 + 1] = [-0.8,  2.0,  1.8, -0.4]
  ```
- `"爱"` 的最终表示：
  ```plaintext
  [-0.4 + 0.8415,  1.6 + 0.9984, -1.0 + 0.01,  0.6 + 1] = [0.4415,  2.5984, -0.99,  1.6]
  ```
- `"学习"` 的最终表示：
  ```plaintext
  [0.6 + 0.9093,  0.4 + 0.9937, -0.2 + 0.02,  1.0 + 1] = [1.5093,  1.3937, -0.18,  2.0]
  ```

结合后的向量表示为：
```plaintext
"我" -> [-0.8,  2.0,  1.8, -0.4]
"爱" -> [0.4415,  2.5984, -0.99,  1.6]
"学习" -> [1.5093,  1.3937, -0.18,  2.0]
```

## 注意力层

### 自注意力

#### 详细解释

注意力机制是一种让模型能够选择性地关注输入数据中重要部分的技术。它模仿了人类的注意力行为 - 当我们阅读一个句子时，我们会更关注与当前词相关的其他词，而不是平等地对待所有词。在传统的序列模型中（如早期的RNN），模型必须将整个输入序列压缩成一个固定长度的向量，这会导致信息瓶颈，特别是对于长序列。注意力机制通过让模型在处理每个输出时都能"查看"整个输入序列，并决定关注哪些部分，解决了这个问题。

注意力的核心公式是：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) \cdot V
$$

其中：
- **$Q$（查询）**：当前处理位置的向量表示
- **$K$（键）**：所有位置的向量表示，用于计算与查询的相关性
- **$V$（值）**：所有位置的向量表示，被加权求和产生输出
- **$d_k$**：键向量的维度，用于缩放

在注意力机制中，**Q（查询）**、**K（键）** 和 **V（值）** 通常通过输入序列 $X$ 和权重矩阵 $W_Q, W_K, W_V$ 的线性变换得到。

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

- **$X$**：输入序列，维度为 $(\text{batch size}, \text{sequence length}, d_{\text{model}})$  
  - $\text{batch size}$：一次处理的样本数（即批大小）
  - $\text{sequence length}$：序列的长度（如句子中的词数）
  - $d_{\text{model}}$：输入特征的维度（如嵌入向量的维度）

- **$W_Q, W_K, W_V$**：权重矩阵  
  - 每个的维度为 $(d_{\text{model}}, d_{\text{model}})$

- **$Q, K, V$**：查询、键和值  
  - 每个的维度为 $(\text{batch size}, \text{sequence length}, d_{\text{model}})$

#### 简单举例

假设我们有一个简单的句子：

```
我喜欢吃苹果。
```

---

##### 1. 词嵌入

经过上文的词嵌入层后生成了一个嵌入矩阵 $X$，其维度为 $(\text{句子长度}, \text{嵌入维度})$：
$$
X =
\begin{bmatrix}
0.2 & 0.4 & 0.6 \\ 
0.8 & 0.1 & 0.5 \\ 
0.3 & 0.7 & 0.9 \\ 
0.5 & 0.2 & 0.1
\end{bmatrix}
$$

##### 2.权重矩阵

假设我们有以下权重矩阵 $W_Q, W_K, W_V$，每个的维度为 $(\text{嵌入维度}, \text{嵌入维度})$：

$W_Q$（查询权重矩阵）将输入序列中的每个词的嵌入向量映射为查询向量 $Q$。查询向量代表了当前词在寻找相关信息时的“问题”。例如，当翻译句子“我喜欢吃苹果”时，当前词“喜欢”会生成一个查询向量，表示“谁与我相关？”。$W_Q$决定模型在计算注意力时“关注什么”。

$W_K$（键权重矩阵）将输入序列中的每个词的嵌入向量映射为键向量 $K$**。键向量类似于一个“索引”或“标签”，表示当前词可以提供的信息。例如，“吃”生成的键向量可能表示“动作相关”的信息。$W_K$决定模型如何匹配查询向量（$Q$）与当前词的相关性。

$W_V$（值权重矩阵）将输入序列中的每个词的嵌入向量映射为值向量 ($V$)。值向量包含实际的“内容”或“答案”，即模型最终会使用的特征信息。例如，“苹果”生成的值向量可能包含“水果”或“食物”相关的信息。$W_V$决定模型最终输出的内容。
$$
W_Q =
\begin{bmatrix}
0.1 & 0.2 & 0.3 \\ 
0.4 & 0.5 & 0.6 \\ 
0.7 & 0.8 & 0.9
\end{bmatrix}, \quad
W_K =
\begin{bmatrix}
0.9 & 0.8 & 0.7 \\ 
0.6 & 0.5 & 0.4 \\ 
0.3 & 0.2 & 0.1
\end{bmatrix}, \quad
W_V =
\begin{bmatrix}
1.0 & 1.1 & 1.2 \\ 
1.3 & 1.4 & 1.5 \\ 
1.6 & 1.7 & 1.8
\end{bmatrix}
$$

##### 3. 计算 Q, K, V

利用矩阵乘法分别计算查询 ($Q$)、键 ($K$) 和值 ($V$)。

$$
Q = X \cdot W_Q =
\begin{bmatrix}
0.2 & 0.4 & 0.6 \\ 
0.8 & 0.1 & 0.5 \\ 
0.3 & 0.7 & 0.9 \\ 
0.5 & 0.2 & 0.1
\end{bmatrix}
\cdot
\begin{bmatrix}
0.1 & 0.2 & 0.3 \\ 
0.4 & 0.5 & 0.6 \\ 
0.7 & 0.8 & 0.9
\end{bmatrix}
=
\begin{bmatrix}
0.62 & 0.76 & 0.90 \\ 
0.50 & 0.66 & 0.82 \\ 
0.96 & 1.25 & 1.54 \\ 
0.33 & 0.41 & 0.49
\end{bmatrix}
$$

$$
K = X \cdot W_K =
\begin{bmatrix}
0.2 & 0.4 & 0.6 \\ 
0.8 & 0.1 & 0.5 \\ 
0.3 & 0.7 & 0.9 \\ 
0.5 & 0.2 & 0.1
\end{bmatrix}
\cdot
\begin{bmatrix}
0.9 & 0.8 & 0.7 \\ 
0.6 & 0.5 & 0.4 \\ 
0.3 & 0.2 & 0.1
\end{bmatrix}
=
\begin{bmatrix}
0.63 & 0.52 & 0.41 \\ 
0.97 & 0.86 & 0.75 \\ 
1.23 & 1.04 & 0.85 \\ 
0.57 & 0.49 & 0.41
\end{bmatrix}
$$

$$
V = X \cdot W_V =
\begin{bmatrix}
0.2 & 0.4 & 0.6 \\ 
0.8 & 0.1 & 0.5 \\ 
0.3 & 0.7 & 0.9 \\ 
0.5 & 0.2 & 0.1
\end{bmatrix}
\cdot
\begin{bmatrix}
1.0 & 1.1 & 1.2 \\ 
1.3 & 1.4 & 1.5 \\ 
1.6 & 1.7 & 1.8
\end{bmatrix}
=
\begin{bmatrix}
1.78 & 1.96 & 2.14 \\ 
1.56 & 1.71 & 1.86 \\ 
2.46 & 2.70 & 2.94 \\ 
0.87 & 0.96 & 1.05
\end{bmatrix}
$$

---

##### 4. 点积注意力机制

点积注意力计算效率高，可以很好地捕捉两个向量之间的相似性。点积注意力可以被理解为一种“问答匹配”机制：查询 ($Q$): 当前的“问题”。键 ($K$): 所有的“答案索引”。值 ($V$): 所有的“答案内容”。通过点积计算查询和键之间的相似度，可以快速找到与当前问题最相关的答案。
$$
QK^\top =
\begin{bmatrix}
0.62 & 0.76 & 0.90 \\ 
0.50 & 0.66 & 0.82 \\ 
0.96 & 1.25 & 1.54 \\ 
0.33 & 0.41 & 0.49
\end{bmatrix}
\cdot
\begin{bmatrix}
0.63 & 0.97 & 1.23 & 0.57 \\ 
0.52 & 0.86 & 1.04 & 0.49 \\ 
0.41 & 0.75 & 0.85 & 0.41
\end{bmatrix}
=
\begin{bmatrix}
1.45 & 2.45 & 3.00 & 1.41 \\ 
1.23 & 2.06 & 2.54 & 1.19 \\ 
2.01 & 3.35 & 4.14 & 1.95 \\ 
0.69 & 1.15 & 1.42 & 0.67
\end{bmatrix}
$$

---

##### 5.缩放

缩放的是为了避免在高维空间中计算点积时，数值过大导致的梯度消失或梯度爆炸问题，从而改善训练的稳定性和性能。

缩放因子为 $\sqrt{d_k} = \sqrt{3} \approx 1.73$，对注意力分数进行缩放：

$$
\text{缩放后的分数} =
\frac{1}{1.73}
\begin{bmatrix}
1.45 & 2.45 & 3.00 & 1.41 \\ 
1.23 & 2.06 & 2.54 & 1.19 \\ 
2.01 & 3.35 & 4.14 & 1.95 \\ 
0.69 & 1.15 & 1.42 & 0.67
\end{bmatrix}
=
\begin{bmatrix}
0.84 & 1.42 & 1.73 & 0.82 \\ 
0.71 & 1.19 & 1.47 & 0.69 \\ 
1.16 & 1.94 & 2.39 & 1.13 \\ 
0.40 & 0.66 & 0.82 & 0.39
\end{bmatrix}
$$

---

##### 6.应用 Softmax

对每一行的分数应用 Softmax 函数：

以第一行为例：
$$
\text{Softmax}([0.84, 1.42, 1.73, 0.82]) =
\left[
\frac{e^{0.84}}{e^{0.84} + e^{1.42} + e^{1.73} + e^{0.82}},
\frac{e^{1.42}}{e^{0.84} + e^{1.42} + e^{1.73} + e^{0.82}},
\frac{e^{1.73}}{e^{0.84} + e^{1.42} + e^{1.73} + e^{0.82}},
\frac{e^{0.82}}{e^{0.84} + e^{1.42} + e^{1.73} + e^{0.82}}
\right]
$$

计算结果：
$$
\text{Softmax 第 1 行} = [0.15, 0.27, 0.37, 0.21]
$$

对所有行重复此步骤，得到 Softmax 矩阵：

$$
\text{Softmax 矩阵} =
\begin{bmatrix}
0.15 & 0.27 & 0.37 & 0.21 \\ 
0.18 & 0.30 & 0.34 & 0.18 \\ 
0.14 & 0.25 & 0.39 & 0.22 \\ 
0.16 & 0.28 & 0.37 & 0.19
\end{bmatrix}
$$

---

##### 7. 计算注意力输出

将 Softmax 矩阵与 $V$ 相乘，得到最终的注意力输出：

$$
\text{Z} =
\begin{bmatrix}
0.15 & 0.27 & 0.37 & 0.21 \\ 
0.18 & 0.30 & 0.34 & 0.18 \\ 
0.14 & 0.25 & 0.39 & 0.22 \\ 
0.16 & 0.28 & 0.37 & 0.19
\end{bmatrix}
\cdot
\begin{bmatrix}
1.78 & 1.96 & 2.14 \\ 
1.56 & 1.71 & 1.86 \\ 
2.46 & 2.70 & 2.94 \\ 
0.87 & 0.96 & 1.05
\end{bmatrix}
$$

最终输出：
$$
\text{Z} =
\begin{bmatrix}
2.15 & 2.37 & 2.59 \\ 
2.11 & 2.33 & 2.55 \\ 
2.23 & 2.46 & 2.68 \\ 
2.19 & 2.42 & 2.64
\end{bmatrix}
$$

### 多头自注意力

```py
# 带掩码多多头注意力层
class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout=0.1):
        super().__init__()

        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)

    def attention(self, q, k, v, d_k, mask=None, dropout=None):
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
        # 处理的过程中，由于句子长度不一样 通常会将句子填充到相同的长度
        # 判断是否提供了掩码
        if mask is not None:
            # 为掩码添加一个维度
            # scores: [批次大小, 头数, 序列长度, 序列长度]
            # 原始mask: [批次大小, 序列长度]
            # 现mask: [批次大小, 1, 序列长度] 允许掩码在广播时应用到所有注意力头
            mask = mask.unsqueeze(1)
            # 将掩码为0的位置在分数矩阵中填充为-1e9 使其经过softmax处理后约为0
            scores = scores.masked_fill(mask == 0, -1e9)
        scores = F.softmax(scores, dim=-1)
        if dropout is not None:
            scores = dropout(scores)
        output = torch.matmul(scores, v)
        return output

    def forward(self, q, k, v, mask=None):
        bs = q.size(0)
        # 利用线性计算划分成h个头
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        # 矩阵转置
        k = k.transpose(1, 2)
        q = q.transpose(1, 2)
        v = v.transpose(1, 2)
        # 计算attention
        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)
        # 连接多个头并输入最后的线性层
        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)
        output = self.out(concat)
        return output
```

#### 详细解释

多头注意力（Multi-Head Attention）是 Transformer 架构的核心组件，它通过多个并行的注意力计算单元（"头"）来捕获输入序列中的不同特征和模式。

单个注意力头只能学习一种特定的关系模式。例如在"我喜欢吃苹果"这个句子中：
- 单头注意力可能只关注"喜欢"和"苹果"之间的语义关系
- 但可能忽略了"我"和"喜欢"之间的主谓关系

多头注意力允许模型同时关注不同类型的关系：
- **头1**可能关注语法结构（如主谓关系）
- **头2**可能关注语义相关性（如"喜欢"和"苹果"的关系）
- **头3**可能关注短程依赖
- **头4**可能关注长程依赖

就像人类在阅读文本时会同时注意多个方面（语法、语义、上下文等）。

多头注意力的数学表达式为：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h) W^O
$$

其中每个头的计算为：

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

- **$h$**: 注意力头的数量（通常为8或16）
- **$d_{model}$**: 输入向量的维度（例如512）
- **$d_k = d_v = d_{model}/h$**: 每个头的维度（例如64）
- **$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$**: 第i个头的查询变换矩阵
- **$W_i^K \in \mathbb{R}^{d_{model} \times d_k}$**: 第i个头的键变换矩阵
- **$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$**: 第i个头的值变换矩阵
- **$W^O \in \mathbb{R}^{hd_v \times d_{model}}$**: 输出变换矩阵

#### 简单举例

##### 1. 初始嵌入矩阵

使用4×4的方阵 $X$ 作为初始嵌入矩阵（代表4个词的嵌入表示，每个词的嵌入维度为4）：

$$
X =
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
$$

我们假设这4行分别代表"我"、"喜欢"、"吃"、"苹果"这四个词的嵌入表示。

##### 2. 多头注意力参数

假设使用2个注意力头，维度 $d_k = d_v = 2$（每个头的输出维度为2）。

其中头1的参数矩阵如下:
$$
W_Q^1 = 
\begin{bmatrix}
0.1 & 0.2 \\
0.3 & 0.4 \\
0.5 & 0.6 \\
0.7 & 0.8
\end{bmatrix}, \quad
W_K^1 = 
\begin{bmatrix}
0.2 & 0.3 \\
0.4 & 0.5 \\
0.6 & 0.7 \\
0.8 & 0.9
\end{bmatrix}, \quad
W_V^1 = 
\begin{bmatrix}
0.1 & 0.2 \\
0.3 & 0.4 \\
0.5 & 0.6 \\
0.7 & 0.8
\end{bmatrix}
$$

##### 3. 计算每个头的Q)、K和V

以头1为例

计算查询矩阵：$Q^1 = X \cdot W_Q^1$
$$
Q^1 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
\cdot
\begin{bmatrix}
0.1 & 0.2 \\
0.3 & 0.4 \\
0.5 & 0.6 \\
0.7 & 0.8
\end{bmatrix}
=
\begin{bmatrix}
0.44 & 0.52 \\
1.12 & 1.36 \\
0.44 & 0.62 \\
0.96 & 1.16
\end{bmatrix}
$$

计算键矩阵：$K^1 = X \cdot W_K^1$
$$
K^1 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
\cdot
\begin{bmatrix}
0.2 & 0.3 \\
0.4 & 0.5 \\
0.6 & 0.7 \\
0.8 & 0.9
\end{bmatrix}
=
\begin{bmatrix}
0.52 & 0.62 \\
1.36 & 1.62 \\
0.62 & 0.82 \\
1.16 & 1.38
\end{bmatrix}
$$

计算值矩阵：$V^1 = X \cdot W_V^1$
$$
V^1 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
\cdot
\begin{bmatrix}
0.1 & 0.2 \\
0.3 & 0.4 \\
0.5 & 0.6 \\
0.7 & 0.8
\end{bmatrix}
=
\begin{bmatrix}
0.44 & 0.52 \\
1.12 & 1.36 \\
0.44 & 0.62 \\
0.96 & 1.16
\end{bmatrix}
$$

##### 4. 计算注意力分数

计算头1的注意力分数

计算注意力分数：$Score^1 = Q^1 \cdot (K^1)^T / \sqrt{d_k}$，其中 $d_k = 2$，$\sqrt{d_k} = 1.414$

$$
Score^1 = \frac{Q^1 \cdot (K^1)^T}{1.414} = \frac{1}{1.414}
\begin{bmatrix}
0.44 & 0.52 \\
1.12 & 1.36 \\
0.44 & 0.62 \\
0.96 & 1.16
\end{bmatrix}
\cdot
\begin{bmatrix}
0.52 & 1.36 & 0.62 & 1.16 \\
0.62 & 1.62 & 0.82 & 1.38
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
0.39 & 1.02 & 0.49 & 0.87 \\
1.00 & 2.64 & 1.27 & 2.23 \\
0.43 & 1.13 & 0.55 & 0.96 \\
0.86 & 2.26 & 1.10 & 1.92
\end{bmatrix}
$$

应用softmax函数（逐行）：

$$
Attention^1 = softmax(Score^1) \approx
\begin{bmatrix}
0.10 & 0.38 & 0.13 & 0.33 \\
0.08 & 0.41 & 0.10 & 0.27 \\
0.11 & 0.37 & 0.14 & 0.31 \\
0.09 & 0.39 & 0.12 & 0.27
\end{bmatrix}
$$

##### 5. 计算加权和输出

$$
O^1 = Attention^1 \cdot V^1 = 
\begin{bmatrix}
0.10 & 0.38 & 0.13 & 0.33 \\
0.08 & 0.41 & 0.10 & 0.27 \\
0.11 & 0.37 & 0.14 & 0.31 \\
0.09 & 0.39 & 0.12 & 0.27
\end{bmatrix}
\cdot
\begin{bmatrix}
0.44 & 0.52 \\
1.12 & 1.36 \\
0.44 & 0.62 \\
0.96 & 1.16
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
0.87 & 1.05 \\
0.91 & 1.11 \\
0.85 & 1.03 \\
0.89 & 1.08
\end{bmatrix}
$$

$$
O^2 = Attention^2 \cdot V^2 \approx
\begin{bmatrix}
0.83 & 0.71 \\
0.87 & 0.74 \\
0.82 & 0.69 \\
0.85 & 0.72
\end{bmatrix}
$$

##### 6. 拼接输出并进行最终线性变换

拼接两个头的输出：

$$
O_{concat} = [O^1; O^2] =
\begin{bmatrix}
0.87 & 1.05 & 0.83 & 0.71 \\
0.91 & 1.11 & 0.87 & 0.74 \\
0.85 & 1.03 & 0.82 & 0.69 \\
0.89 & 1.08 & 0.85 & 0.72
\end{bmatrix}
$$

假设输出投影矩阵 $W^O$ 为：

$$
W^O = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
$$

最终多头注意力输出：

$$
MultiHead(Q, K, V) = O_{concat} \cdot W^O
$$

$$
= 
\begin{bmatrix}
0.87 & 1.05 & 0.83 & 0.71 \\
0.91 & 1.11 & 0.87 & 0.74 \\
0.85 & 1.03 & 0.82 & 0.69 \\
0.89 & 1.08 & 0.85 & 0.72
\end{bmatrix}
\cdot
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\
0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 & 0.7
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
1.42 & 0.86 & 1.12 & 1.39 \\
1.48 & 0.90 & 1.17 & 1.45 \\
1.40 & 0.84 & 1.09 & 1.36 \\
1.45 & 0.88 & 1.14 & 1.41
\end{bmatrix}
$$

## 前馈层

```py
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout=0.1):
        super().__init__()

        # d_ff 默认设为 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        x = self.dropout(F.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x
```

### 详细解释

前馈神经网络层（Feed-Forward Network，FFN）是Transformer架构中的关键组成部分，位于多头自注意力机制之后。它对每个位置的表示进行独立的相同处理，为模型增加非线性变换能力。

前馈层由两个线性变换和一个激活函数组成：

$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$

1. **第一个线性层（扩展层）**：
   - 作用：将输入的特征维度从 $d_{\text{model}}$ 扩展到 $d_{\text{ff}}$。
   - 通常情况下，$d_{\text{ff}}$ 是 $d_{\text{model}}$ 的 4 倍。例如，如果 $d_{\text{model}} = 512$，则 $d_{\text{ff}} = 2048$。
   - 扩展维度的目的是增加模型容量，从而能够学习更复杂的特征。

2. **激活函数（ReLU）**：
   - 作用：对扩展后的特征应用 ReLU（Rectified Linear Unit）激活函数。
   - ReLU 的功能是引入非线性变换，将所有负值置为 $0$，正值保持不变。这种非线性变换使模型能够学习更加复杂的映射关系。

3. **第二个线性层（压缩层）**：
   - 作用：将特征维度从 $d_{\text{ff}}$ 压缩回 $d_{\text{model}}$。
   - 压缩特征的目的是将高维空间聚合的信息映射回原始维度，便于与后续模块进行整合。

**增大前馈层隐状态的维度有利于提高最终翻译结果的质量。**多头注意力层负责捕获序列内的关系和依赖。**前馈层**则是将这些特征进行进一步处理和转换。两者结合提供了Transformer强大的建模能力。

### 简单举例

假设多头注意力的输出是矩阵：
$$
\mathbf{X} =
\begin{bmatrix}
1.42 & 0.86 & 1.12 & 1.39 \\
1.48 & 0.90 & 1.17 & 1.45 \\
1.40 & 0.84 & 1.09 & 1.36 \\
1.45 & 0.88 & 1.14 & 1.41
\end{bmatrix}
$$
我们将此矩阵作为输入，经过前馈层的处理。

我们假设前馈层的参数如下：
- 第一个线性层权重矩阵 $\mathbf{W}_1$ 和偏置 $\mathbf{b}_1$：
  $$
  \mathbf{W}_1 \in \mathbb{R}^{4 \times 8}, \quad \mathbf{b}_1 \in \mathbb{R}^{8}
  $$
- 第二个线性层权重矩阵 $\mathbf{W}_2$ 和偏置 $\mathbf{b}_2$：
  $$
  \mathbf{W}_2 \in \mathbb{R}^{8 \times 4}, \quad \mathbf{b}_2 \in \mathbb{R}^{4}
  $$

假设：
$$
\mathbf{W}_1 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\
0.9 & 1.0 & 1.1 & 1.2 & 1.3 & 1.4 & 1.5 & 1.6 \\
1.7 & 1.8 & 1.9 & 2.0 & 2.1 & 2.2 & 2.3 & 2.4 \\
2.5 & 2.6 & 2.7 & 2.8 & 2.9 & 3.0 & 3.1 & 3.2 
\end{bmatrix}, \quad
\mathbf{b}_1 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8
\end{bmatrix}
$$

$$
\mathbf{W}_2 = 
\begin{bmatrix}
0.5 & 0.4 & 0.3 & 0.2 \\
0.1 & 0.9 & 0.8 & 0.7 \\
0.6 & 0.5 & 0.4 & 0.3 \\
0.2 & 0.1 & 0.9 & 0.8 \\
0.7 & 0.6 & 0.5 & 0.4 \\
0.3 & 0.2 & 0.1 & 0.9 \\
0.8 & 0.7 & 0.6 & 0.5 \\
0.4 & 0.3 & 0.2 & 0.1
\end{bmatrix}, \quad
\mathbf{b}_2 = 
\begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4
\end{bmatrix}
$$



将输入 $\mathbf{X}$ 与 $\mathbf{W}_1$ 相乘并加上偏置 $\mathbf{b}_1$：

$$
\mathbf{Z}_1 = \mathbf{X} \cdot \mathbf{W}_1 + \mathbf{b}_1
$$

结果：
$$
\mathbf{Z}_1 =
\begin{bmatrix}
8.78 & 9.88 & 10.98 & 12.08 & 13.18 & 14.28 & 15.38 & 16.48 \\
9.14 & 10.34 & 11.54 & 12.74 & 13.94 & 15.14 & 16.34 & 17.54 \\
8.66 & 9.76 & 10.86 & 11.96 & 13.06 & 14.16 & 15.26 & 16.36 \\
8.98 & 10.08 & 11.18 & 12.28 & 13.38 & 14.48 & 15.58 & 16.68
\end{bmatrix}
$$

应用 ReLU 激活函数，将负值置为 $0$，正值保持不变：

$$
\mathbf{A}_1 = \text{ReLU}(\mathbf{Z}_1)
$$

结果：
$$
\mathbf{A}_1 =
\begin{bmatrix}
8.78 & 9.88 & 10.98 & 12.08 & 13.18 & 14.28 & 15.38 & 16.48 \\
9.14 & 10.34 & 11.54 & 12.74 & 13.94 & 15.14 & 16.34 & 17.54 \\
8.66 & 9.76 & 10.86 & 11.96 & 13.06 & 14.16 & 15.26 & 16.36 \\
8.98 & 10.08 & 11.18 & 12.28 & 13.38 & 14.48 & 15.58 & 16.68
\end{bmatrix}
$$

将激活后的结果 $\mathbf{A}_1$ 与 $\mathbf{W}_2$ 相乘并加上偏置 $\mathbf{b}_2$，获得最终的结果：

$$
\mathbf{Z}_2 = \mathbf{A}_1 \cdot \mathbf{W}_2 + \mathbf{b}_2
$$

结果：
$$
\mathbf{Z}_2 =
\begin{bmatrix}
191.68 & 230.54 & 269.40 & 308.26 \\
199.24 & 239.56 & 279.88 & 320.20 \\
189.92 & 228.34 & 266.76 & 305.18 \\
196.28 & 235.90 & 275.52 & 315.14
\end{bmatrix}
$$

## 残差连接

### 详细解释

残差连接（Residual Connection）是深度学习中一种重要的技术，用于解决深度神经网络训练中的 **梯度消失** 和 **退化问题**。它的主要思想是让网络学习 **残差**，而不是直接学习复杂的映射。**主要是指使用一条直连通道直接将对应子层的输入连接到输出，避免了在优化过程中因网络过深而产生的潜在的梯度消失问题。**

数学公式表示为：
$$H(x) = F(x) + x$$

- **$x$**：输入
- **$F(x)$**：残差函数，表示需要学习的部分
- **$H(x)$**：输出

在Transformer中，残差连接用于连接自注意力层和前馈网络层：

$$\text{LayerNorm}(x + \text{MultiHeadAttention}(x))$$
$$\text{LayerNorm}(x + \text{FeedForward}(x))$$

这种设计促进了梯度流动并保持了特征信息，对于构建超深语言模型至关重要。

## 层归一化

```py
class Norm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()

        self.size = d_model

        # 层归一化包含两个可学习参数
        self.alpha = nn.Parameter(torch.ones(self.size))  # 缩放参数
        self.bias = nn.Parameter(torch.zeros(self.size))  # 偏移参数

        self.eps = eps  # 避免除零

    def forward(self, x):
        # 计算均值
        mean = x.mean(dim=-1, keepdim=True)
        # 计算标准差
        std = x.std(dim=-1, keepdim=True)
        # 应用归一化公式
        norm = self.alpha * (x - mean) / (std + self.eps) + self.bias
        return norm
```

### 详细解释

层归一化（Layer Normalization）是一种正则化技术，用于提高深度神经网络的训练稳定性和收敛速度，被广泛用于 Transformer 的编码器和解码器层中，特别是在自注意力和前馈网络的输出后。与批归一化（Batch Normalization）不同，层归一化的核心是对输入的每一层神经元的输出进行归一化处理，使其均值为 0，方差为 1，以减少神经元之间的协方差偏移问题。主要用于序列建模任务（如自然语言处理和时间序列分析）中。**层归一化可以有效的缓解优化过程潜在的不稳定、收敛速度慢等问题。**

假设输入为 $x \in \mathbb{R}^{B \times T \times D}$，其中：
- $B$ 是批量大小（batch size）
- $T$ 是序列长度
- $D$ 是每个特征向量的维度

对于某一特定样本的特征向量 $x_i$，层归一化的计算公式为：

$$
\text{Norm}(x_i) = \alpha \cdot \frac{x_i - \mu}{\sigma + \epsilon} + \beta
$$

其中：
- $\mu$ 是特征的均值：$\mu = \frac{1}{D} \sum_{j=1}^D x_{ij}$
- $\sigma$ 是特征的标准差：$\sigma = \sqrt{\frac{1}{D} \sum_{j=1}^D (x_{ij} - \mu)^2}$
- $\epsilon$ 是一个很小的值，用来避免分母为零
- $\alpha$ 和 $\beta$ 是可学习的参数，分别用于缩放和偏移

| **特性**            | **层归一化（LayerNorm）**          | **批归一化（BatchNorm）**              |
| ------------------- | ---------------------------------- | -------------------------------------- |
| **归一化维度**      | 对每个样本的特征维度进行归一化     | 对一个批次的样本进行归一化             |
| **适用场景**        | 自然语言处理、序列建模             | 图像任务、传统深度学习                 |
| **依赖批量大小**    | 不依赖批量大小（单样本也能归一化） | 依赖批量大小，批量太小可能效果较差     |
| **训练/推理一致性** | 训练和推理阶段的归一化方式相同     | 推理阶段需要使用移动平均估计均值和方差 |
| **计算开销**        | 更适合小批量或在线学习             | 对大批量更高效                         |



