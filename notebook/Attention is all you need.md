# Attention is all you need

## 摘要

目前主流的序列转换模型基于复杂的循环或卷积神经网络，包含编码器和解码器。性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，彻底摒弃了循环和卷积结构。在两项机器翻译任务上的实验表明，这些模型在质量上更为出色，同时具有更高的并行性，并且所需的训练时间显著减少。我们的模型在WMT 2014英德翻译任务上达到了28.4 BLEU分，比现有的最佳结果（包括集成模型）提高了超过2个BLEU分。在WMT 2014英法翻译任务上，我们的模型在8个GPU上训练3.5天后建立了新的单模型最高水平，BLEU分数达到41.0，仅用了文献中最佳模型训练成本的一小部分。

## 1 介绍

循环神经网络，尤其是长短期记忆网络[12]和门控循环神经网络[7]，已经被牢固地确立为序列建模和转换问题（如语言建模和机器翻译）的最先进方法[29, 2, 5]。此后，众多研究工作继续推动循环语言模型和编码器-解码器架构的边界[31, 21, 13]。

循环模型通常沿着输入和输出序列的符号位置进行计算分解。通过将位置与计算时间的步骤对齐，它们生成一系列隐藏状态 $$h_t$$，这些状态是前一个隐藏状态 $$h_{t-1}$$ 和位置 t 的输入的函数。这种**固有的顺序性质阻碍了训练样本内的并行化**，而在较长序列长度时，这一点变得尤为关键，因为**内存限制约束了跨样本的批处理**。最近的工作通过分解技巧[18]和条件计算[26]在计算效率方面取得了显著提高，同时在后者的情况下还改进了模型性能。然而，顺序计算的基本限制仍然存在。

注意力机制已经成为各种任务中引人注目的序列建模和转换模型的不可或缺的部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2, 16]。然而，除了少数情况[22]外，这些注意力机制通常与循环网络结合使用。

在本研究中，我们提出了Transformer，**这是一种摒弃循环结构，而完全依赖注意力机制来构建输入和输出之间全局依赖关系的模型架构**。Transformer允许**更高程度的并行化**，并且在仅用八块P100 GPU训练十二小时后就能达到机器翻译领域的新水平。

## 2 背景

减少顺序计算的目标也构成了扩展神经GPU[20]、ByteNet[15]和ConvS2S[8]的基础，这些模型都使用卷积神经网络作为基本构建块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联两个任意输入或输出位置信号所需的操作数随着位置之间的距离而增长，ConvS2S呈线性增长，ByteNet呈对数增长。这使得学习远距离位置之间的依赖关系变得更加困难[11]。**在Transformer中，这被减少到恒定数量的操作，尽管代价是由于对注意力加权位置进行平均而导致有效分辨率降低**，我们通过3.2节中描述的多头注意力(Multi-Head Attention)来抵消这种影响。

**自注意力(Self-attention)**，有时也称为内部注意力(intra-attention)，是一种**将单个序列的不同位置相互关联以计算序列表示的注意力机制**。自注意力已在多种任务中成功应用，包括阅读理解、抽象摘要、文本蕴含以及学习与任务无关的句子表示[4, 22, 23, 19]。

**端到端记忆网络(End-to-end memory networks)**基于循环注意力机制而非序列对齐的循环，已被证明在简单语言问答和语言建模任务上表现良好[28]。

然而，据我们所知，**Transformer是第一个完全依靠自注意力计算其输入和输出表示而不使用序列对齐RNN或卷积的转换模型**。在接下来的章节中，我们将描述Transformer，论述自注意力的动机，并讨论它相对于[14, 15]和[8]等模型的优势。

## 3 模型架构

目前最具竞争力的神经序列转换模型都采用**编码器-解码器**结构[5, 2, 29]。在这种结构中，编码器将符号表示的输入序列$(x_1, ..., x_n)$映射为连续表示序列$z = (z_1, ..., z_n)$。给定$z$，解码器随后逐个元素地生成输出符号序列$(y_1, ..., y_m)$。在每一步，模型是**自回归的**[9]，**在生成下一个符号时将先前生成的符号作为额外输入。**

Transformer遵循这种整体架构，对编码器和解码器都使用堆叠的自注意力层和逐点的全连接层，分别如图1的左半部分和右半部分所示。

![image-20250418162609888](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250418162609888.png)

### 3.1 编码器和解码器堆叠

**编码器**：编码器由$N = 6$个相同层堆叠而成。每一层有两个子层。第一个是多头自注意力机制，第二个是简单的、逐位置的全连接前馈网络。我们**在每个子层周围采用残差连接**[10]，然后进行**层标准化**[1]。也就是说，每个子层的输出是$\text{LayerNorm}(x + \text{Sublayer}(x))$，其中$\text{Sublayer}(x)$是由子层本身实现的函数。为了便于这些残差连接，模型中的所有子层以及嵌入层都产生维度为$d_{model} = 512$的输出。

**解码器**：解码器同样由$N = 6$个相同层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后进行层标准化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注后续位置。**这种遮蔽(masking)，结合输出嵌入偏移一个位置的事实，确保了位置$i$的预测只能依赖于位置小于$i$的已知输出。**

### 3.2 注意力

注意力函数可以描述为将查询(query)和一组键值对(key-value pairs)映射到输出的过程，其中查询、键、值和输出都是向量。输出计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数计算得出。

#### 3.2.1 缩放点积注意力

我们称我们特定的注意力机制为"缩放点积注意力"(Scaled Dot-Product Attention)(图2)。输入包括维度为$d_k$的查询和键，以及维度为$d_v$的值。我们计算查询与所有键的点积，将每个点积除以$\sqrt{d_k}$，然后应用softmax函数以获得值的权重。

在实践中，我们同时计算一组查询的注意力函数，将它们打包成矩阵$Q$。键和值也被分别打包成矩阵$K$和$V$。我们计算输出矩阵的方式为：

$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \tag{1}$$

两种最常用的注意力函数是加性注意力[2]和点积(乘性)注意力。点积注意力与我们的算法相同，除了缩放因子$\frac{1}{\sqrt{d_k}}$。加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论复杂度上相似，但**点积注意力在实践中更快且空间效率更高，因为它可以使用高度优化的矩阵乘法代码实现**。

虽然对于较小的$d_k$值，这两种机制表现相似，但对于较大的$d_k$值，不进行缩放的点积注意力表现不如加性注意力[3]。**我们推测，对于较大的$d_k$值，点积的幅值会变得很大，将softmax函数推入具有极小梯度的区域⁴。为了抵消这种效果，我们通过$\frac{1}{\sqrt{d_k}}$缩放点积。**

****

注:为了说明为什么点积会变得很大，假设$q$和$k$的分量是均值为0、方差为1的独立随机变量。那么它们的点积$q \cdot k = \sum_{i=1}^{d_k} q_i k_i$的均值为0，方差为$d_k$。

****

![image-20250418164117900](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250418164117900.png)

#### 3.2.2 多头注意力

我们发现，与其使用$d_{model}$维的键、值和查询执行单个注意力函数，不如**将查询、键和值分别使用不同的、可学习的线性投影h次投影到$d_k$、$d_k$和$d_v$维。**对这些查询、键和值的投影版本，我们并行执行注意力函数，得到$d_v$维的输出值。这些输出被连接起来，并再次进行投影，产生最终的值，如图2所示。

**多头注意力允许模型同时关注来自不同位置的不同表示子空间的信息。**使用单个注意力头时，平均化会抑制这种能力。

$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

其中 $\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$

这里的投影是参数矩阵$W^Q_i \in \mathbb{R}^{d_{model} \times d_k}$、$W^K_i \in \mathbb{R}^{d_{model} \times d_k}$、$W^V_i \in \mathbb{R}^{d_{model} \times d_v}$和$W^O \in \mathbb{R}^{hd_v \times d_{model}}$。

在本工作中，我们使用$h = 8$个并行注意力层，即头。对于每个头，我们使用$d_k = d_v = d_{model}/h = 64$。由于每个头的维度减小，总计算成本与使用完整维度的单头注意力相似。

#### 3.2.3 注意力在我们模型中的应用

Transformer模型以三种不同的方式使用多头注意力：

• 在"编码器-解码器注意力"层中，查询来自前一个解码器层，而记忆的键和值来自编码器的输出。这允许解码器中的每个位置关注输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，例如[31, 2, 8]。

• 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器前一层的输出。编码器中的每个位置都可以关注编码器前一层的所有位置。

• 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流以保持自回归特性。我们通过在缩放点积注意力内部将所有对应于非法连接的softmax输入值掩蔽(设置为$-\infty$)来实现这一点。参见图2。

### 3.3 基于位置的前馈网络

除了注意力子层外，我们的编码器和解码器的每一层都包含一个全连接前馈网络，该网络分别且相同地应用于每个位置。这由两个线性变换组成，中间有一个ReLU激活函数。

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 \tag{2}$$

虽然线性变换在不同位置上是相同的，但它们在不同层使用不同的参数。另一种描述方式是将其视为核大小为1的两个卷积。输入和输出的维度是$d_{model} = 512$，内层的维度是$d_{ff} = 2048$。

### 3.4 嵌入和Softmax

与其他序列转导模型类似，我们使用可学习的嵌入将输入标记和输出标记转换为$d_{model}$维的向量。我们还使用常规的可学习线性变换和softmax函数将解码器输出转换为预测下一个标记的概率。在我们的模型中，两个嵌入层和softmax前的线性变换共享相同的权重矩阵，类似于[24]。在嵌入层中，我们将这些权重乘以$\sqrt{d_{model}}$。

### 3.5 位置编码

表1：不同层类型的最大路径长度、每层复杂度和最小顺序操作数。n是序列长度，d是表示维度，k是卷积的核大小，r是受限自注意力中的邻域大小。

| 层类型         | 每层复杂度    | 顺序操作 | 最大路径长度 |
| -------------- | ------------- | -------- | ------------ |
| 自注意力       | O(n² · d)     | O(1)     | O(1)         |
| 循环           | O(n · d²)     | O(n)     | O(n)         |
| 卷积           | O(k · n · d²) | O(1)     | O(logₖ(n))   |
| 自注意力(受限) | O(r · n · d)  | O(1)     | O(n/r)       |

由于我们的模型不包含循环和卷积，为了让模型能够利用序列中标记的顺序信息，我们必须注入一些关于序列中标记的相对或绝对位置的信息。为此，我们在编码器和解码器堆栈的底部向输入嵌入添加"位置编码"。位置编码具有与嵌入相同的维度$d_{model}$，这样两者可以相加。位置编码有多种选择，包括可学习的和固定的[8]。

在本工作中，我们使用不同频率的正弦和余弦函数：

$$\text{PE}(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)$$

$$\text{PE}(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)$$

其中pos是位置，i是维度。也就是说，位置编码的每个维度对应一个正弦曲线。波长形成从$2\pi$到$10000 \cdot 2\pi$的几何级数。我们选择这个函数是因为我们假设它能让模型容易地学习按相对位置进行注意力计算，因为对于任何固定偏移量k，$\text{PE}_{\text{pos}+k}$可以表示为$\text{PE}_{\text{pos}}$的线性函数。

我们还尝试了使用可学习的位置嵌入[8]，发现  在各个方面的差异，这些层通常用于将一个可变长度的符号表示序列$(x_1,...,x_n)$映射到另一个等长序列$(z_1,...,z_n)$，其中$x_i, z_i \in \mathbb{R}^d$，例如典型序列转导编码器或解码器中的隐藏层。为了说明我们使用自注意力的动机，我们考虑三个期望的特性。

一个是每层的总计算复杂度。另一个是可并行化的计算量，用所需的最小顺序操作数来衡量。

第三个是网络中长距离依赖之间的路径长度。学习长距离依赖是许多序列转导任务中的关键挑战。影响学习此类依赖能力的一个关键因素是信号在网络中前向和后向传播必须经过的路径长度。输入和输出序列中任意位置组合之间的这些路径越短，就越容易学习长距离依赖[11]。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意力层通过恒定数量的顺序执行操作连接所有位置，而循环层需要O(n)个顺序操作。就计算复杂度而言，当序列长度n小于表示维度d时，自注意力层比循环层更快，这在机器翻译中使用的句子表示（如word-piece[31]和byte-pair[25]表示）的最先进模型中通常是这种情况。为了提高处理非常长序列任务的计算性能，自注意力可以被限制为仅考虑输入序列中以相应输出位置为中心的大小为r的邻域。这将使最大路径长度增加到O(n/r)。我们计划在未来的工作中进一步研究这种方法。

单个卷积层如果卷积核宽度k < n，则无法连接所有的输入和输出位置对。要做到这一点，在连续卷积核的情况下需要O(n/k)层卷积堆叠，或在扩张卷积[15]的情况下需要O(logₖ(n))层，这增加了网络中任意两个位置之间最长路径的长度。卷积层通常比循环层更昂贵，因子为k。然而，可分离卷积[6]能显著降低复杂度，至O(k·n·d+n·d²)。不过，即使k = n，可分离卷积的复杂度也等同于自注意力层与逐点前馈层的组合，这正是我们在模型中采用的方法。

作为额外好处，自注意力可以产生更具可解释性的模型。我们检查了模型中的注意力分布，并在附录中展示和讨论了例子。**不仅个别注意力头明显学会执行不同的任务，许多注意力头似乎表现出与句子的句法和语义结构相关的行为。**

## 5 训练

本节描述了我们模型的训练方案。

### 5.1 训练数据与批处理

我们在标准WMT 2014英德数据集上进行训练，该数据集包含约450万个句子对。句子使用字节对编码（byte-pair encoding）[3]进行编码，共享的源语言-目标语言词汇表大约有37000个标记。对于英法翻译，我们使用了明显更大的WMT 2014英法数据集，其包含3600万个句子，并将标记分割为32000个词片（word-piece）词汇表[31]。句子对按照近似序列长度进行批处理。每个训练批次包含一组句子对，大约包含25000个源语言标记和25000个目标语言标记。

### 5.2 硬件与时间安排

我们在一台装有8个NVIDIA P100 GPU的机器上训练了我们的模型。对于使用本文描述的超参数的基础模型，每个训练步骤大约需要0.4秒。我们总共训练基础模型100,000步，约12小时。对于我们的大型模型（描述在表3的最后一行），每步时间为1.0秒。大型模型训练了300,000步（3.5天）。

### 5.3 优化器

我们使用Adam优化器[17]，参数为β₁ = 0.9，β₂ = 0.98和ε = 10⁻⁹。我们在训练过程中根据以下公式调整学习率：

$$\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5})$$

这相当于在前warmup_steps个训练步骤中线性增加学习率，之后按步数的平方根的倒数比例减少。我们使用warmup_steps = 4000。

### 5.4 正则化

我们在训练期间使用三种类型的正则化：

**残差丢弃** 我们对每个子层的输出应用丢弃（dropout）[27]，然后再将其添加到子层输入并进行归一化。此外，我们对编码器和解码器堆栈中的嵌入和位置编码的总和应用丢弃。对于基础模型，我们使用的丢弃率为$P_{drop} = 0.1$。

**标签平滑** 在训练期间，我们采用了值为$\epsilon_{ls} = 0.1$的标签平滑[30]。这会损害模型的困惑度，因为模型学会了更加不确定，但它改善了准确率和BLEU分数。

表2: Transformer模型以远低于之前的训练成本达到了比先前最先进模型更好的BLEU分数，测试集为英德和英法newstest2014。

![image-20250418171113041](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250418171113041.png)

## 6 结果

### 6.1 机器翻译

在WMT 2014英语到德语翻译任务上，大型Transformer模型（表2中的Transformer (big)）的性能超过了先前报告的最佳模型（包括集成模型）2.0多个BLEU分，创造了28.4的新BLEU分数记录。该模型的配置列在表3的最后一行。训练在8个P100 GPU上进行，耗时3.5天。即使是我们的基础模型也超过了所有先前发布的模型和集成模型，而且训练成本只是任何竞争模型的一小部分。

在WMT 2014英语到法语翻译任务上，我们的大型模型达到了41.0的BLEU分数，超过了所有先前发布的单一模型，而训练成本不到先前最先进模型的1/4。用于英法翻译的Transformer (big)模型使用了$P_{drop} = 0.1$的丢弃率，而不是0.3。

对于基础模型，我们使用了通过平均最后5个检查点（每10分钟记录一次）得到的单一模型。对于大型模型，我们平均了最后20个检查点。我们使用束搜索，束大小为4，长度惩罚$\alpha = 0.6$ [31]。这些超参数是在开发集上实验后选择的。我们在推理过程中将最大输出长度设置为输入长度+50，但在可能的情况下提前终止[31]。

表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、使用的GPU数量和每个GPU的估计持续单精度浮点计算能力相乘来估计训练模型所使用的浮点运算次数⁵。

****

注:我们分别对K80、K40、M40和P100使用了2.8、3.7、6.0和9.5 TFLOPS的值。

****

### 6.2 模型变体

为了评估Transformer不同组件的重要性，我们以不同方式调整了基础模型，并测量了在英德翻译开发集（newstest2013）上性能的变化。我们使用了上一节描述的束搜索方法，但没有进行检查点平均。我们在表3中展示了这些结果。

在表3的(A)行中，我们变化了注意力头的数量以及注意力键和值的维度，同时保持计算量不变，如第3.2.2节所述。虽然单头注意力比最佳设置差0.9个BLEU分，但头数过多时质量也会下降。

表3: Transformer架构的变体。未列出的值与基础模型相同。所有指标都基于英德翻译开发集newstest2013。列出的困惑度是按照我们的字节对编码的每词片(per-wordpiece)困惑度，不应与每词(per-word)困惑度进行比较。

![image-20250418171816521](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250418171816521.png)

在表3的(B)行中，我们观察到减小注意力键的大小$d_k$会损害模型质量。这表明确定兼容性并不容易，可能需要比点积更复杂的兼容性函数。我们在(C)和(D)行中进一步观察到，正如预期的那样，更大的模型效果更好，而丢弃对于避免过拟合非常有帮助。在(E)行中，我们用学习到的位置嵌入[8]替换了我们的正弦位置编码，观察到与基础模型几乎相同的结果。

## 结论

在本工作中，我们提出了Transformer，这是第一个完全基于注意力的序列转换模型，它用多头自注意力机制取代了编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer的训练速度明显快于基于循环或卷积层的架构。在WMT 2014英语到德语和WMT 2014英语到法语翻译任务上，我们都达到了新的技术水平。在前一项任务中，我们的最佳模型甚至超过了所有先前报告的集成模型。

我们对基于注意力的模型的未来感到兴奋，并计划将它们应用于其他任务。我们计划将Transformer扩展到涉及文本以外的输入和输出模态的问题，并研究局部的、受限的注意力机制，以有效处理大型输入和输出，如图像、音频和视频。使生成过程减少顺序依赖是我们的另一个研究目标。

我们用于训练和评估模型的代码可在https://github.com/tensorflow/tensor2tensor上获取。

## 致谢

我们感谢Nal Kalchbrenner和Stephan Gouws富有成效的评论、修正和启发。