# BERT: 用于语言理解的深度双向Transformer预训练模型

## 摘要

我们介绍了一种新的语言表示模型，名为BERT，全称为Bidirectional Encoder Representations from Transformers（**双向Transformer编码表示**）。与最近的语言表示模型（Peters等，2018a；Radford等，2018）不同，**BERT旨在通过在所有层中同时考虑左右上下文，从未标记文本中预训练深度双向表示。**因此，**预训练好的BERT模型只需通过一个额外的输出层进行微调，即可创建适用于广泛任务的最先进模型**，如问答和语言推理，而无需进行实质性的特定任务架构修改。

BERT在概念上简单且实证上强大。它在十一项自然语言处理任务上取得了新的最先进结果，包括将GLUE分数提升至80.5%（7.7个百分点的绝对提升），MultiNLI准确率提升至86.7%（4.6个百分点的绝对提升），SQuAD v1.1问答测试的F1分数提升至93.2（1.5点的绝对提升）以及SQuAD v2.0测试的F1分数提升至83.1（5.1点的绝对提升）。

## 1 引言

语言模型预训练已被证明能有效改进许多自然语言处理任务（Dai和Le，2015；Peters等，2018a；Radford等，2018；Howard和Ruder，2018）。这些任务包括句子级任务，如自然语言推理（Bowman等，2015；Williams等，2018）和释义（Dolan和Brockett，2005），这些任务旨在通过整体分析句子来预测句子间的关系；还包括词元级任务，如命名实体识别和问答，这些任务要求模型在词元级别产生细粒度输出（Tjong Kim Sang和De Meulder，2003；Rajpurkar等，2016）。

目前应用预训练语言表示到下游任务的策略有两种：基于特征的方法和微调方法。基于特征的方法，如ELMo（Peters等，2018a），使用包含预训练表示作为额外特征的特定任务架构。微调方法，如生成式预训练Transformer（OpenAI GPT）（Radford等，2018），引入最少的特定任务参数，并通过简单地微调所有预训练参数来训练下游任务。这两种方法在预训练期间共享相同的目标函数，即使用单向语言模型来学习通用语言表示。

我们认为当前技术限制了预训练表示的能力，尤其是对于微调方法而言。主要限制在于标准语言模型是单向的，这限制了在预训练期间可使用的架构选择。例如，在OpenAI GPT中，作者使用了从**左到右的架构**，即每个词元在Transformer的自注意力层中只能关注前面的词元（Vaswani等，2017）。这种限制**对于句子级任务而言是次优**的，并且**在将基于微调的方法应用于词元级任务（如问答）时可能非常有害**，因为在这些任务中，**融合两个方向的上下文至关重要**。

在本文中，我们通过提出BERT:来自变压器的双向编码器表示来改进基于微调的方法。BERT通过使用受完形填空任务启发的“掩码语言模型”(MLM)预训练目标减轻了前面提到的单向性约束(Taylor, 1953)。掩码语言模型随机屏蔽输入中的一些标记，其目标是仅根据其上下文预测被屏蔽词的原始词汇表id。与左右语言模型预训练不同，MLM目标使表示能够融合左右上下文，这允许我们预训练深度双向transformer。除了掩码语言模型之外，我们还使用了“下一个句子预测”任务来联合预训练文本对表示。本文的贡献如下:

在本文中，我们通过提出BERT（双向Transformer编码表示）改进了基于微调的方法。BERT通过使用受完形填空任务（Taylor，1953）启发的**"掩码语言模型"（MLM）**预训练目标，解除了前述的单向性约束。**掩码语言模型随机掩盖输入中的一些词元，其目标是仅基于上下文预测被掩盖词的原始词汇ID。**与从左到右的语言模型预训练不同，MLM目标使表示能够**融合左右上下文**，这使我们能够预训练深度双向Transformer。除掩码语言模型外，我们还使用"下一句预测"任务来联合预训练文本对表示。本文的贡献如下：

• 我们证明了双向预训练对语言表示的重要性。与Radford等（2018）使用单向语言模型进行预训练不同，**BERT使用掩码语言模型来实现预训练的深度双向表示。**这也与Peters等（2018a）形成对比，后者使用独立训练的从左到右和从右到左语言模型的浅层连接。

• 我们表明**预训练表示减少了对许多精心设计的特定任务架构的需求。**BERT是第一个在大量句子级和词元级任务上达到最先进性能的基于微调的表示模型，其表现优于许多特定任务架构。

• BERT提升了十一项NLP任务的最先进水平。代码和预训练模型可在 https://github.com/google-research/bert 获取。

## 2 相关工作

预训练通用语言表示有着悠久的历史，本节将简要回顾最广泛使用的方法。

### 2.1 无监督基于特征的方法

**学习广泛适用的词**表示几十年来一直是活跃的研究领域，包括非神经网络方法（Brown等，1992；Ando和Zhang，2005；Blitzer等，2006）和神经网络方法（Mikolov等，2013；Pennington等，2014）。预训练词嵌入是现代NLP系统不可或缺的部分，与从头开始学习的嵌入相比，它提供了显著的改进（Turian等，2010）。为了预训练词嵌入向量，研究人员使用了从左到右的语言建模目标（Mnih和Hinton，2009），以及区分左右上下文中正确词和错误词的目标（Mikolov等，2013）。

这些方法已经被扩展到更粗粒度的应用，如句子嵌入（Kiros等，2015；Logeswaran和Lee，2018）或段落嵌入（Le和Mikolov，2014）。为了训练句子表示，先前的研究采用了各种目标，包括对候选下一句进行排序（Jernite等，2017；Logeswaran和Lee，2018），基于前一句表示从左到右生成下一句的词（Kiros等，2015），或基于去噪自编码器的目标（Hill等，2016）。

ELMo及其前身（Peters等，2017，2018a）沿着不同维度推广了传统词嵌入研究。它们从从左到右和从右到左的语言模型中提取上下文敏感特征。每个词元的上下文表示是从左到右和从右到左表示的连接。当将上下文词嵌入与现有的特定任务架构集成时，ELMo提升了几个主要NLP基准测试的最先进水平（Peters等，2018a），包括问答（Rajpurkar等，2016）、情感分析（Socher等，2013）和命名实体识别（Tjong Kim Sang和De Meulder，2003）。Melamud等（2016）提出通过预测单个词的任务来学习上下文表示，该任务使用LSTM从左右上下文预测单词。与ELMo类似，他们的模型是基于特征的，而非深度双向的。Fedus等（2018）表明，完形填空任务可用于提高文本生成模型的鲁棒性。

### 2.2 无监督微调方法

与基于特征的方法一样，该方向的最早工作仅从未标记文本中预训练词嵌入参数（Collobert和Weston，2008）。

更近期，产生上下文词元表示的句子或文档编码器已从未标记文本中预训练，并为监督下游任务进行微调（Dai和Le，2015；Howard和Ruder，2018；Radford等，2018）。这些方法的优势在于只需从头学习很少的参数。至少部分由于这一优势，OpenAI GPT（Radford等，2018）在GLUE基准（Wang等，2018a）的许多句子级任务上取得了当时最先进的结果。从左到右的语言建模和自编码器目标已被用于预训练此类模型（Howard和Ruder，2018；Radford等，2018；Dai和Le，2015）。

### 2.3 从监督数据中进行迁移学习

研究还表明，从具有大型数据集的监督任务进行有效迁移的可能性，例如自然语言推理（Conneau等，2017）和机器翻译（McCann等，2017）。计算机视觉研究也证明了从大型预训练模型进行迁移学习的重要性，其中一个有效的方法是微调通过ImageNet预训练的模型（Deng等，2009；Yosinski等，2014）。

![image-20250419094520122](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419094520122.png)

**图1：BERT的整体预训练和微调过程。**除了输出层外，在预训练和微调中使用相同的架构。相同的预训练模型参数用于初始化不同下游任务的模型。在微调过程中，所有参数都会被微调。[CLS]是添加在每个输入示例前面的特殊符号，[SEP]是特殊的分隔符标记（例如，用于分隔问题/答案）。

## 3 BERT

我们在本节介绍BERT及其详细实现。我们的框架包含两个步骤：预训练和微调。**在预训练过程中，模型通过不同的预训练任务在未标记数据上进行训练。对于微调，BERT模型首先使用预训练参数进行初始化，然后使用来自下游任务的标记数据对所有参数进行微调。**每个下游任务都有单独的微调模型，尽管它们都使用相同的预训练参数进行初始化。图1中的问答示例将作为本节的持续示例。

BERT的一个显著特点是其跨不同任务的统一架构。预训练架构与最终下游架构之间的差异最小。

### 模型架构

BERT的模型架构是一个**多层双向Transformer编码器**，基于Vaswani等（2017）描述的原始实现，并在tensor2tensor库中发布。¹ 由于Transformer的使用已经变得普遍，且我们的实现与原始实现几乎相同，我们将省略模型架构的详尽背景描述，读者可参考Vaswani等（2017）以及"The Annotated Transformer"²等优秀指南。

> [!NOTE]
>
> 3. #### **结构**
>    每一层 Transformer 编码器由以下模块组成：
>    
>    1. **多头自注意力机制（Multi-Head Self-Attention）**
>       - 每个词元可以通过注意力机制关注序列中所有其他词元（双向自注意力）。
>       - 多个注意力头用于捕获不同的关系和上下文信息。
>    
>    2. **残差连接 + 层归一化（Residual Connection + Layer Normalization）**
>       - **残差连接**：将子层的输入直接加到其输出上，缓解梯度消失问题。
>       - **层归一化**：对每个词元的特征进行标准化，提升训练稳定性。
>    
>    3. **前馈网络 (Feed-Forward Network, FFN)**
>       - 包括两个全连接层：
>         - 第一层扩展隐藏层维度为 $$4 \times hidden\_size$$。
>         - 第二层将维度还原为 $$hidden\_size$$。
>       - 激活函数通常使用 **GELU**（Gaussian Error Linear Unit）。

在本研究中，我们将**层数（即Transformer块）表示为L**，**隐藏层大小为H**，**自注意力头数为A**。³ 我们主要报告两种模型大小的结果：
- BERTBASE（L=12，H=768，A=12，总参数=110M）
- BERTLARGE（L=24，H=1024，A=16，总参数=340M）

选择BERTBASE与OpenAI GPT具有相同的模型大小是为了比较目的。然而，关键区别在于**BERT Transformer使用双向自注意力，而GPT Transformer使用有约束的自注意力，即每个词元只能关注其左侧的上下文。⁴**

> [!IMPORTANT]
>
> 网络结构上可以参考上一个文章。双向注意力还是单向注意力使用掩码实现。

> [!NOTE]
>
> 双向自注意力与受约束的自注意力
>
> 1. 限制的单向多头注意力
>
> - 单向注意力限制每个词元只能关注其 **左侧的上下文（前文）**。
> - 每个词元无法访问其 **右侧的上下文（后文）**，以避免信息泄漏。
> - 这种机制特别适合文本生成任务，例如 GPT，生成当前词时不能提前看到后续词。
>
> 实现步骤
>
> 1. **输入序列**：`["I", "love", "coding"]`
> 2. 每个词元 \( Q \) 的查询向量只能与其 **左侧词元的键向量 \( K \)** 计算相关性。
> 3. 通过注意力掩码（Mask）屏蔽掉右侧的词元，使右侧的注意力分数为负无穷（在 softmax 中归一化为 0）。
>
> #### **计算示例**
> 假设输入序列长度为 3，注意力分数矩阵 \( QK^T \) 为：
> $$
> \begin{bmatrix}
> q_1 \cdot k_1 & q_1 \cdot k_2 & q_1 \cdot k_3 \\
> q_2 \cdot k_1 & q_2 \cdot k_2 & q_2 \cdot k_3 \\
> q_3 \cdot k_1 & q_3 \cdot k_2 & q_3 \cdot k_3 \\
> \end{bmatrix}
> $$
>
> 添加掩码后：
> $$
> \begin{bmatrix}
> q_1 \cdot k_1 & -\infty & -\infty \\
> q_2 \cdot k_1 & q_2 \cdot k_2 & -\infty \\
> q_3 \cdot k_1 & q_3 \cdot k_2 & q_3 \cdot k_3 \\
> \end{bmatrix}
> $$
>
> - 第1行：`"I"` 只能关注自身。
> - 第2行：`"love"` 只能关注 `"I"` 和自身。
> - 第3行：`"coding"` 只能关注 `"I"`, `"love"` 和自身。
>
> ---
>
> 2. 双向多头注意力
>
> 定义
>
> - 双向注意力允许每个词元同时关注序列中 **左侧和右侧的上下文**。
> - 每个词元的查询向量 \( Q \) 可以与整个序列的键向量 \( K \) 计算相关性。
> - 这种机制适合理解型任务，例如 BERT，能捕获全局上下文。
>
> 实现步骤
>
> 1. **输入序列**：`["I", "love", "coding"]`
> 2. 每个词元 \( Q \) 的查询向量与序列中所有词元的键向量 \( K \) 计算相关性，没有方向性限制。
>
> #### **计算示例**
> 假设输入序列长度为 3，注意力分数矩阵 \( QK^T \) 为：
> $$
> \begin{bmatrix}
> q_1 \cdot k_1 & q_1 \cdot k_2 & q_1 \cdot k_3 \\
> q_2 \cdot k_1 & q_2 \cdot k_2 & q_2 \cdot k_3 \\
> q_3 \cdot k_1 & q_3 \cdot k_2 & q_3 \cdot k_3 \\
> \end{bmatrix}
> $$
>
> - 第1行：`"I"` 可以关注 `"I"`, `"love"`, `"coding"`。
> - 第2行：`"love"` 可以关注 `"I"`, `"love"`, `"coding"`。
> - 第3行：`"coding"` 可以关注 `"I"`, `"love"`, `"coding"`。
>

### 输入/输出表示

为了让BERT能够处理多种任务，我们的输入格式需要能够清楚地表示单个句子或句子对（例如，<问题, 答案>）。这里所说的“**句子**”其实可以是**任何一段连续的文本**，并不一定是严格意义上的语言学句子。而“**序列**”指的是**BERT的输入数据**，它可以是一个单独的句子，也可以是两个句子组合起来的序列。

> [!NOTE]
>
> 句子举例
>
> "I love cats."  "What is your name?"  "The quick brown fox jumps over the lazy dog." "This is a paragraph. It contains multiple sentences." 
>
> 序列
>
> ```
> # 单独的句子序列
> [CLS] I love cats . [SEP]
> # 句子对序列
> [CLS] What is your name ? [SEP] My name is John . [SEP]
> ```

我们使用具有30,000词元词汇表的WordPiece嵌入（Wu等，2016）。**每个序列的第一个词元始终是特殊的分类词元（[CLS]）**。与此词元对应的最终隐藏状态用作分类任务的聚合序列表示。**句子对被打包成单个序列**。我们通过两种方式区分这些句子。首先，我们**用特殊词元（[SEP]）分隔它们**。其次，我们**为每个词元添加一个学习型嵌入，表明它属于句子A**还是句子B。如图1所示，我们将输入嵌入表示为E，特殊[CLS]词元的最终隐藏向量表示为$$C \in \mathbb{R}^H$$，第i个输入词元的最终隐藏向量表示为$$T_i \in \mathbb{R}^H$$。

对于给定的词元，其输入表示是通过对相应的词元、段落和位置嵌入求和构建的。这种构建的可视化可见图2。

![image-20250419100421674](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419100421674.png)

**图2：BERT输入表示。** 输入嵌入是词元嵌入、分段嵌入和位置嵌入的总和。

> [!IMPORTANT]
>
> 假设批量大小为 4，句子最大长度为 512，隐藏层维度为 768。
>
> | 序列编号 | 原始句子           | 分词序列                        |
> | -------- | ------------------ | ------------------------------- |
> | 1        | "I love cats."     | `[CLS] I love cats . [SEP]`     |
> | 2        | "Hello world!"     | `[CLS] Hello world ! [SEP]`     |
> | 3        | "BERT is amazing." | `[CLS] BERT is amazing . [SEP]` |
> | 4        | "How are you?"     | `[CLS] How are you ? [SEP]`     |
>
> ### **(1) 词元嵌入（Token Embeddings）**
>
> - 将输入文本分词为词元（Tokens），包括特殊标记 `[CLS]` 和 `[SEP]`，每个词元映射为一个固定维度的向量。
> - 词表大小通常为约30,000（使用WordPiece词汇表）。
> - 每个句子被补齐（Padding）到最大长度 512。
>
> ```
> # 假设分此后结果如下
> [101, 146, 1367, 4215, 102, 0, 0, ..., 0]  # 序列1
> [101, 7592, 2088, 999, 102, 0, 0, ..., 0]  # 序列2
> [101, 14324, 2003, 6429, 1012, 102, 0, ..., 0]  # 序列3
> [101, 2129, 2024, 2017, 1029, 102, 0, ..., 0]  # 序列4
> ```
>
> ### **(2) 段嵌入（Segment Embeddings）**
> - 用于区分单个句子和句子对任务中的两个句子。
> - 两种段嵌入：
>   - **Segment A（句子A）**：第一个句子。
>   - **Segment B（句子B）**：第二个句子。
> - 示例：
>   - 对于句子对任务，"What is your name?" 和 "My name is John."：
>     - `["[CLS]", "What", "is", "your", "name", "?", "[SEP]", "My", "name", "is", "John", ".", "[SEP]"]`
>     - `Segment A`: `[0, 0, 0, 0, 0, 0, 0]`
>     - `Segment B`: `[1, 1, 1, 1, 1, 1]`
>
> ```
> # 单句任务 所有段嵌入为0
> [0, 0, 0, 0, 0, 0, ..., 0]  # 序列1
> [0, 0, 0, 0, 0, 0, ..., 0]  # 序列2
> [0, 0, 0, 0, 0, 0, ..., 0]  # 序列3
> [0, 0, 0, 0, 0, 0, ..., 0]  # 序列4
> ```
>
> ### **(3) 位置嵌入（Position Embeddings）**
> - 表示词元在序列中的相对位置。
> - **BERT使用可学习的向量来表示位置**，而不是Transformer中的正弦位置编码。
> - 序列长度固定为512（不足时填充`[PAD]`，超过时截断）。
>
> 可学习的位置表示：
>
> 每个位置（从0到最大序列长度512）都有一个唯一的向量表示，称为位置嵌入。这些嵌入向量的维度与词嵌入（Token Embedding）的维度相同。在模型初始化时，这些位置嵌入向量被随机初始化为参数。通过反向传播，位置嵌入会逐渐学习到如何更好地表示位置信息，以支持模型的各种下游任务。
>
> ```
> # 随机生成
> [0, 1, 2, 3, 4, 5, ..., 511]  # 所有序列共享相同的位置索引
> ```
>
> 词元嵌入、段嵌入的维度都是[4, 512, 768],段嵌入利用广播机制也可进行叠加。
>
> - 最终，每个词元的嵌入表示为：
>   ```
>   词元嵌入 + 位置嵌入 + 段嵌入

### 3.1 预训练BERT

与Peters等（2018a）和Radford等（2018）不同，我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用两个**无监督任务**来预训练BERT，本节将对此进行描述。这一步骤在图1的左侧部分展示。

**任务#1：掩码语言模型（Masked LM）** 直观上，合理的推断是深度双向模型严格强于从左到右模型或从左到右与从右到左模型的浅层连接。不幸的是，标准条件语言模型只能从左到右或从右到左训练，因为双向条件会使每个词间接地"看到自己"，模型可以在多层上下文中轻易预测目标词。

为了训练深度双向表示，我们简单地随机掩盖输入词元的某个百分比，然后预测这些被掩盖的词元。我们将这个过程称为"掩码语言模型"（MLM），尽管在文献中它通常被称为完形填空任务（Taylor，1953）。在这种情况下，**与掩码词元相对应的最终隐藏向量被输入到词汇表上的输出softmax中**，就像在标准语言模型中一样。在我们所有的实验中，我们随机掩盖每个序列中15%的WordPiece词元。与去噪自编码器（Vincent等，2008）不同，我们**只预测被掩盖的词**，而不是重构整个输入。

虽然这使我们能够获得双向预训练模型，但缺点是我们在**预训练和微调之间创建了不匹配，因为[MASK]词元在微调过程中不会出现。**为了缓解这一问题，我们不总是用实际的[MASK]词元替换"被掩盖"的词。训练数据生成器**随机选择15%的词元位置进行预测。**如果选择了第i个词元，我们会：**(1) 80%的情况下用[MASK]词元替换第i个词元；(2) 10%的情况下用随机词元替换；(3) 10%的情况下保持第i个词元不变。**然后，$$T_i$$将被用于预测原始词元，使用**交叉熵**损失。我们在附录C.2中比较了这个过程的变体。

> [!NOTE]
>
> ### 掩码语言模型（Masked LM）总结
>
> 1. **目的**：通过随机掩盖词元，训练深度双向表示，捕获上下文的双向信息。
>
> 2. **训练过程**：
>    - 随机选择 15% 的词元进行掩盖。
>    - 只预测被掩盖的词元，而非重构整个输入。
>    - 掩盖策略：
>      - 80% 用 `[MASK]` 替换。
>      - 10% 用随机词元替换。
>      - 10% 保持不变。
>
> 3. **损失函数**：使用交叉熵损失预测原始词元。
>
> 4. **优点与问题**：
>    - **优点**：实现双向预训练。
>    - **问题**：微调时不存在 `[MASK]`，导致预训练与微调不匹配。
>
> 5. **解决方案**：采用混合掩盖策略（部分随机替换或保持不变）。
>
> 6. **与去噪自编码器的区别**：只预测被掩盖词元，而非重构整个输入。

**任务#2：下一句预测（NSP）** 许多重要的下游任务，如问答（QA）和自然语言推理（NLI），都基于**理解两个句子之间的关系**，而这并不能通过语言建模直接捕获。为了训练一个理解句子关系的模型，我们预训练了一个二元化的下一句预测任务，该任务可以从任何单语语料库中轻松生成。具体来说，**当为每个预训练示例选择句子A和B时，50%的情况下B是实际上跟在A后面的下一个句子（标记为IsNext），50%的情况下B是从语料库中随机选择的句子（标记为NotNext）**。如图1所示，$$C$$用于下一句预测（NSP）。 尽管这很简单，但我们在5.1节中证明，针对这一任务的预训练对QA和NLI都非常有益。NSP任务与Jernite等（2017）和Logeswaran和Lee（2018）中使用的表示学习目标密切相关。然而，在**先前的工作中，只有句子嵌入被转移到下游任务，而BERT则转移所有参数来初始化终端任务模型参数。**

> [!NOTE]
>
> ### 下一句预测（NSP）
>
> BERT中的下一句预测（NSP）任务通过以下步骤实现：
>
> 1. **数据准备**：
>    
>    - 从文本语料中抽取连续的句子对(句子A,句子B)
>    - 构建正负样本：
>      - 50%情况：句子B是句子A的真实后续句子（标记为IsNext）
>      - 50%情况：句子B是随机选择的句子（标记为NotNext）
>    
> 2. **输入表示**：
>    
>    ```
>    [CLS] 句子A [SEP] 句子B [SEP]
>    ```
>    - 使用特殊标记[CLS]开始整个序列
>    - 使用[SEP]分隔两个句子
>    - 添加位置编码(Position Embedding)
>    - 添加段落编码(Segment Embedding)：
>      - 句子A中所有词元使用相同的段落编码(通常为0)
>      - 句子B中所有词元使用另一种段落编码(通常为1)
>    
> 3. **预测过程**：
>    
>    - 将整个序列输入BERT模型
>    - 提取[CLS]标记的最终隐藏状态向量
>    - 通过一个简单的分类层(线性层+softmax)预测IsNext或NotNext
>
> 严格来说，NSP不是传统意义上的"无监督学习"，而是**自监督学习**，但常被归类为广义的无监督学习，因为无需人工标注，可以利用文本内在结构学习通用表示。
>

**预训练数据** 预训练过程在很大程度上遵循现有的语言模型预训练文献。对于预训练语料库，我们使用了BooksCorpus（8亿词）（Zhu等，2015）和英语维基百科（25亿词）。对于维基百科，我们只提取文本段落，而忽略列表、表格和标题。使用文档级语料库而非像Billion Word Benchmark（Chelba等，2013）那样的打乱句子级语料库是至关重要的，这样才能提取长的连续序列。

### 3.2 微调BERT

微调很简单，因为Transformer中的自注意力机制允许BERT建模许多下游任务——无论是涉及单一文本还是文本对——只需交换适当的输入和输出即可。对于涉及文本对的应用，一种常见模式是在应用双向交叉注意力之前独立编码文本对，如Parikh等（2016）；Seo等（2017）。**BERT则使用自注意力机制统一了这两个阶段，因为用自注意力编码连接的文本对，有效地包含了两个句子之间的双向交叉注意力。**

**对于每个任务，我们只需将特定任务的输入和输出插入BERT中，并端到端地微调所有参数。**在输入方面，预训练中的句子A和句子B类似于：(1)复述任务中的句子对；(2)蕴含任务中的假设-前提对；(3)问答任务中的问题-段落对；以及(4)文本分类或序列标注中的退化文本-∅对。在输出方面，词元表示被输入到输出层用于词元级任务，如序列标注或问答，而[CLS]表示被输入到输出层用于分类任务，如蕴含或情感分析。

与预训练相比，**微调相对便宜**。本论文中的所有结果都可以在单个Cloud TPU上最多1小时内复现，或在GPU上几个小时内复现，从完全相同的预训练模型开始。^7 我们在第4节的相应小节中描述了特定任务的细节。更多细节可以在附录A.5中找到。

> [!NOTE]
>
> 维持原有的模型结构不变，然后添加任务专用层。
>
> - **文本分类任务**
>   - 提取`[CLS]`标记的最终隐藏状态向量（通常是768维）
>   - 添加一个简单的分类层（线性变换+softmax）连接到该向量上
>   - 输出为类别数量大小的向量
>   
> - **序列标注任务**（命名实体识别、词性标注等）
>   - 提取每个词元的最终隐藏状态向量
>   - 在每个词元表示上添加标记预测层（线性变换+softmax）
>   - 输出为序列长度×标签数量的矩阵
>   
> - **问答任务**
>   - 利用每个词元的最终隐藏状态向量
>   - 添加两个独立的线性层分别预测答案的起始和结束位置
>   - 输出为两个长度等于序列长度的向量（起始位置概率和结束位置概率）
>
> 然后构造所需的数据
>
> - **单句任务**（文本分类、序列标注）
>   ```
>   [CLS] 输入文本 [SEP]
>   ```
>
> - **句对任务**（文本匹配、自然语言推理等）
>   ```
>   [CLS] 文本A [SEP] 文本B [SEP]
>   ```
>
> - **问答任务**（阅读理解等）
>   ```
>   [CLS] 问题 [SEP] 文本段落 [SEP]
>   ```
>
> 接下来就是训练以及使用一些策略防止过拟合，同时可以选择微调策略。
>
> - **全参数微调**：更新所有参数，包括预训练层和任务层
> - **特征提取**：冻结预训练层，只训练新添加的任务层
> - **分层微调**：只更新最后几层Transformer层和任务层
> - **渐进式解冻**：先训练任务层，然后逐层解冻预训练层

## 4 实验

在本节中，我们将展示BERT在11个NLP任务上的微调结果。

### 4.1 GLUE

通用语言理解评估（GLUE）基准（Wang等，2018a）是多样化自然语言理解任务的集合。GLUE数据集的详细描述包含在附录B.1中。

为了在GLUE上进行微调，我们按照第3节中描述的方式表示输入序列（单个句子或句子对），并使用与第一个输入词元（[CLS]）对应的最终隐藏向量$$C \in \mathbb{R}^H$$作为聚合表示。在微调期间引入的唯一新参数是分类层权重$$W \in \mathbb{R}^{K \times H}$$，其中$$K$$是标签的数量。我们使用$$C$$和$$W$$计算标准分类损失，即$$\log(\text{softmax}(CW^T))$$。

对于所有GLUE任务，我们使用32的批量大小并对数据微调3个周期。对于每个任务，我们在开发集上选择了最佳微调学习率（在5e-5、4e-5、3e-5和2e-5中选择）。此外，对于BERT$_{\text{LARGE}}$，我们发现在小数据集上微调有时不稳定，因此我们运行了几次随机重启并在开发集上选择了最佳模型。在随机重启中，我们使用相同的预训练检查点，但执行不同的微调数据洗牌和分类器层初始化。$^9$

结果呈现在表1中。BERT$_{\text{BASE}}$和BERT$_{\text{LARGE}}$在所有任务上都大幅优于所有系统，相比之前的最佳水平分别获得了4.5%和7.0%的平均准确度提升。值得注意的是，BERT$_{\text{BASE}}$和OpenAI GPT在模型架构方面几乎相同，除了注意力掩码机制。对于规模最大且广泛报道的GLUE任务MNLI，BERT获得了4.6%的绝对准确度提升。在官方GLUE排行榜$^1$$^0$上，BERT$_{\text{LARGE}}$获得了80.5分，而截至撰写时，OpenAI GPT获得了72.8分。

我们发现BERT$_{\text{LARGE}}$在所有任务上都显著优于BERT$_{\text{BASE}}$，尤其是在那些训练数据很少的任务上。模型大小的效果将在5.2节中更详细地探讨。

![image-20250419101558477](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419101558477.png)

**表1：GLUE测试结果**，由评估服务器评分（https://gluebenchmark.com/leaderboard）。
每个任务下方的数字表示训练样例的数量。"平均"列与官方GLUE分数略有不同，因为我们排除了有问题的WNLI集合。$^8$ BERT和OpenAI GPT是单模型、单任务的。QQP和MRPC报告的是F1分数，STS-B报告的是Spearman相关系数，其他任务报告的是准确度分数。我们排除了使用BERT作为组件之一的条目。

### 4.2 SQuAD v1.1

斯坦福问答数据集（SQuAD v1.1）是一个包含10万对众包问题/答案的集合（Rajpurkar等人，2016年）。给定一个问题和包含答案的维基百科段落，任务是预测段落中的答案文本跨度。

如图1所示，在问答任务中，我们将输入问题和段落表示为单个打包序列，问题使用A嵌入，而段落使用B嵌入。我们仅在微调期间引入起始向量$S \in \mathbb{R}^H$和结束向量$E \in \mathbb{R}^H$。词语i是答案跨度起始位置的概率计算为$T_i$和$S$之间的点积，然后对段落中所有词语进行softmax：
$$
P_i = \frac{e^{S·T_i}}{\sum_j e^{S·T_j}}
$$

类似的公式用于答案跨度的结束位置。从位置$i$到位置$j$的候选跨度的分数定义为$S·T_i + E·T_j$，并且使用$j ≥ i$的最高得分跨度作为预测。训练目标是正确起始和结束位置的对数似然之和。我们使用5e-5的学习率和32的批量大小微调3个周期。

表2显示了排行榜顶部条目以及顶尖已发表系统的结果（Seo等人，2017年；Clark和Gardner，2018年；Peters等人，2018a年；Hu等人，2018年）。SQuAD排行榜上的顶尖结果没有可用的最新公开系统描述，$^{11}$并且在训练系统时允许使用任何公开数据。
因此，我们在系统中使用适度的数据增强，通过首先在TriviaQA（Joshi等人，2017年）上微调，然后再在SQuAD上微调。

我们表现最好的系统在集成方面优于排行榜顶级系统+1.5 F1分，作为单一系统时优于+1.3 F1分。事实上，我们的单一BERT模型在F1分数方面优于顶级集成系统。没有TriviaQA微调数据，我们只损失0.1-0.4 F1分，仍然大幅优于所有现有系统。$^{12}$

![image-20250419102316133](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419102316133.png)

**表2：SQuAD 1.1结果。**BERT集成是使用不同预训练检查点和微调种子的7个系统。

![image-20250419102346873](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419102346873.png)

**表3：SQuAD 2.0结果。**我们排除了使用BERT作为其组件之一的条目。

### 4.3 SQuAD v2.0

SQuAD 2.0任务扩展了SQuAD 1.1的问题定义，允许在提供的段落中可能不存在简短答案，使问题更加贴近现实。

我们使用一种简单的方法来扩展SQuAD v1.1 BERT模型以适应这项任务。我们将没有答案的问题视为答案跨度的起始和结束位置都在[CLS]标记处。起始和结束答案跨度位置的概率空间扩展为包括[CLS]标记的位置。对于预测，我们比较无答案跨度的分数：$s_{null} = S·C + E·C$与最佳非空跨度的分数$\hat{s}_{i,j} = \max_{j\geq i}S·T_i + E·T_j$。当$\hat{s}_{i,j} > s_{null} + \tau$时，我们预测一个非空答案，其中阈值$\tau$在开发集上选择以最大化F1分数。我们没有为此模型使用TriviaQA数据。我们使用5e-5的学习率和48的批量大小微调了2个周期。

与之前的排行榜条目和顶尖已发表工作（Sun等人，2018年；Wang等人，2018b年）相比的结果显示在表3中，不包括使用BERT作为其组件之一的系统。我们观察到比之前最佳系统提高了+5.1 F1分数。

### 4.4 SWAG

"具有对抗性生成的情境"（SWAG）数据集包含113k个句子对补全示例，用于评估基于常识的推理（Zellers等人，2018年）。给定一个句子，任务是在四个选项中选择最合理的后续内容。

在SWAG数据集上微调时，我们构建四个输入序列，每个序列包含给定句子（句子A）和可能的后续内容（句子B）的连接。引入的唯一特定于任务的参数是一个向量，其与[CLS]标记表示$C$的点积表示每个选项的分数，通过softmax层进行归一化。

我们使用2e-5的学习率和16的批量大小对模型进行了3个周期的微调。结果在表4中呈现。BERT$_{LARGE}$优于作者的基线ESIM+ELMo系统+27.1%，优于OpenAI GPT 8.3%。

![image-20250419155426191](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419155426191.png)

**表4：SWAG开发集和测试集准确率。**人类表现是通过100个样本测量的，正如SWAG论文中所报告的。

![image-20250419103213291](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419103213291.png)

**表5：使用BERT$_{BASE}$架构对预训练任务进行的消融实验。**"No NSP"是在没有下一句预测任务的情况下训练的。"LTR & No NSP"是作为左到右语言模型训练的，没有下一句预测，类似OpenAI GPT。"+ BiLSTM"在微调期间在"LTR + No NSP"模型顶部添加了一个随机初始化的BiLSTM。

## 5 消融研究

在本节中，我们对BERT的多个方面进行消融实验，以更好地理解它们的相对重要性。额外的消融研究可以在附录C中找到。

> [!NOTE]
>
> 消融实验是一种通过系统性地移除（"消融"）模型中的组件或特性，来分析这些组件对整体模型性能影响的实验方法。这种方法旨在回答"模型中的哪些部分真正有效"这一关键问题。其目的在于：
>
> 1. **验证组件必要性**：确定模型中各个组件是否真正贡献了性能提升
> 2. **量化组件贡献**：测量每个组件或技术对最终结果的具体贡献程度
> 3. **理解模型行为**：增强对模型内部工作机制的理解
> 4. **避免过度复杂化**：识别可能被移除而不影响性能的冗余组件
> 5. **支持研究主张**：为论文中提出的创新点提供实证支持
>
> 有许多的经典方法：
>
> 1. 组件移除法
>
> - 从完整模型中逐一移除或替换特定组件
> - 对比移除前后的性能变化
> - 例：移除注意力机制、跳跃连接或正则化层
>
> 2. 特征消融
>
> - 移除或替换输入特征的某些部分
> - 评估特定特征对模型性能的贡献
> - 例：在图像分类中移除颜色通道，测试模型对形状和纹理的依赖
>
> 3. 架构简化
>
> - 逐步简化模型架构（如减少层数或神经元数量）
> - 找到复杂度和性能的平衡点
> - 例：比较不同深度的网络性能
>
> 4. 训练策略消融
>
> - 移除或修改特定训练技术
> - 评估这些技术对最终性能的影响
> - 例：移除数据增强、学习率调度或早停策略
>



### 5.1 预训练任务的影响

我们通过评估两种预训练目标来证明BERT深度双向性的重要性，这些目标使用与BERT$_{BASE}$完全相同的预训练数据、微调方案和超参数：

**No NSP：**一个使用"masked LM"（MLM）训练但没有"下一句预测"（NSP）任务的双向模型。

**LTR&No NSP：**一个仅使用左侧上下文的模型，它使用标准的从左到右（LTR）语言模型进行训练，而不是MLM。在微调时也应用了仅左侧上下文的约束，因为移除它会引入预训练/微调不匹配，从而降低下游性能。此外，这个模型在预训练时没有NSP任务。这与OpenAI GPT直接可比，但使用了我们更大的训练数据集、我们的输入表示和我们的微调方案。

我们首先研究NSP任务带来的影响。在表5中，我们展示了移除NSP会显著降低QNLI、MNLI和SQuAD 1.1的性能。接下来，我们通过比较"No NSP"和"LTR & No NSP"来评估训练双向表示的影响。LTR模型在所有任务上的表现都比MLM模型差，在MRPC和SQuAD上下降幅度更大。

对于SQuAD，从直觉上很明显LTR模型在标记预测方面表现不佳，因为标记级隐藏状态没有右侧上下文。为了真诚地尝试加强LTR系统，我们在顶部添加了一个随机初始化的BiLSTM。这确实显著提高了SQuAD的结果，但结果仍远远不如预训练双向模型。BiLSTM反而降低了GLUE任务的性能。

我们认识到，也可以训练单独的LTR（从左到右）和RTL（从右到左）模型，并将每个标记表示为两个模型的连接，就像ELMo那样。然而：(a)这比单个双向模型的计算成本高一倍；(b)对于像QA这样的任务，这种方法不直观，因为RTL模型将无法基于问题来条件化答案；(c)这严格来说比深度双向模型能力弱，因为深度双向模型可以在每一层同时使用左右上下文。

### 5.2 模型大小的影响

在本节中，我们探讨模型大小对微调任务准确性的影响。我们训练了多个具有不同层数、隐藏单元和注意力头数的BERT模型，同时使用与之前描述相同的超参数和训练过程。

表6中显示了在选定GLUE任务上的结果。在此表中，我们报告了5次微调随机重启的开发集平均准确性。我们可以看到，更大的模型在所有四个数据集上都带来了严格的准确性提升，即使对于仅有3,600个标记训练样本且与预训练任务有实质性差异的MRPC也是如此。也许令人惊讶的是，我们能够在已经相对于现有文献相当大的模型基础上实现如此显著的改进。例如，Vaswani等人（2017）探索的最大Transformer是（L=6，H=1024，A=16），编码器有1亿参数，而我们在文献中发现的最大Transformer是（L=64，H=512，A=2），有2.35亿参数（Al-Rfou等人，2018）。相比之下，BERT$_{BASE}$包含1.1亿参数，BERT$_{LARGE}$包含3.4亿参数。

长期以来，人们已知增加模型规模会持续改进机器翻译和语言建模等大规模任务的性能，这一点从表6中展示的训练数据留出部分的语言模型困惑度可以看出。然而，我们认为这是首次令人信服地证明，扩展到极端模型规模也能在非常小规模的任务上带来巨大改进，前提是模型已经经过充分的预训练。Peters等人（2018b）在增加预训练双向语言模型规模从两层到四层对下游任务影响方面呈现了混合结果，而Melamud等人（2016）顺便提到将隐藏维度大小从200增加到600有所帮助，但进一步增加到1,000并未带来更多改进。这两项先前的工作都使用了基于特征的方法——我们假设当模型直接在下游任务上微调，并且只使用极少数随机初始化的额外参数时，即使下游任务数据非常少，特定任务的模型也能从更大、更具表现力的预训练表示中获益。

### 5.3 基于特征的BERT方法

到目前为止所呈现的所有BERT结果都使用了微调方法，即在预训练模型上添加一个简单的分类层，并在下游任务上联合微调所有参数。然而，基于特征的方法（从预训练模型中提取固定特征）具有某些优势。首先，并非所有任务都可以通过Transformer编码器架构轻松表示，因此需要添加特定任务的模型架构。其次，预先计算训练数据的昂贵表示一次，然后在此表示之上运行许多使用更廉价模型的实验，这样做有重大的计算优势。

在本节中，我们通过将BERT应用于CoNLL-2003命名实体识别（NER）任务（Tjong Kim Sang和De Meulder，2003）来比较这两种方法。在BERT的输入中，我们使用保留大小写的WordPiece模型，并包括数据提供的最大文档上下文。按照标准做法，我们将其表述为标记任务，但在输出中不使用CRF层。我们使用第一个子标记的表示作为输入，输入到NER标签集的标记级分类器中。

为了对微调方法进行消融实验，我们通过从一个或多个层中提取激活值而不微调BERT的任何参数，应用基于特征的方法。这些上下文嵌入被用作随机初始化的两层768维BiLSTM的输入，然后进入分类层。

结果在表7中呈现。BERT$_{LARGE}$与最先进的方法相比表现具有竞争力。性能最佳的方法连接了预训练Transformer的顶部四个隐藏层的标记表示，这仅比微调整个模型低0.3 F1。这表明BERT对于微调和基于特征的方法都是有效的。

![image-20250419104234565](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419104234565.png)

**表6：BERT模型大小的消融实验。**#L = 层数；#H = 隐藏层大小；#A = 注意力头数。"LM (ppl)"是留出训练数据的掩码语言模型困惑度。

![image-20250419104318342](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419104318342.png)

**表7：CoNLL-2003命名实体识别结果。**超参数是使用开发集选择的。报告的开发集和测试集分数是使用这些超参数的5次随机重启的平均值。

## 结论

由于语言模型转移学习带来的近期经验性改进表明，丰富的、无监督预训练是许多语言理解系统的重要组成部分。特别是，这些结果使即使是低资源任务也能从深度单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构，使同一个预训练模型能够成功处理广泛的NLP任务。

## 附录

我们将附录分为三个部分：

• BERT的额外实现细节在附录A中呈现；

• 我们实验的额外细节在附录B中呈现；以及

• 额外的消融研究在附录C中呈现。

我们为BERT呈现额外的消融研究，包括：

– 训练步骤数量的影响；以及

– 不同掩码程序的消融实验。

### 附录A BERT的额外细节

#### A.1 预训练任务的说明

我们在下文中提供预训练任务的示例。

**掩码语言模型和掩码程序** 假设无标签句子是"my dog is hairy"（我的狗很毛茸茸的），在随机掩码程序中我们选择了第4个标记（对应于"hairy"），我们的掩码程序可以进一步说明为：

• 80%的情况：用[MASK]标记替换该词，例如，my dog is hairy → my dog is [MASK]

• 10%的情况：用随机词替换该词，例如，my dog is hairy → my dog is apple

• 10%的情况：保持词不变，例如，my dog is hairy → my dog is hairy。这样做的目的是使表示偏向实际观察到的词。

这种程序的优势在于Transformer编码器不知道它将被要求预测哪些词或哪些词已被随机词替换，因此它被迫保持每个输入标记的分布式上下文表示。此外，由于随机替换仅发生在所有标记的1.5%（即15%的10%），这似乎不会损害模型的语言理解能力。在C.2节中，我们评估了这个程序的影响。

与标准语言模型训练相比，掩码语言模型在每个批次中只对15%的标记进行预测，这表明模型可能需要更多的预训练步骤才能收敛。在C.1节中，我们证明MLM确实比从左到右的模型（预测每个标记）收敛稍慢，但MLM模型的经验改进远远超过了增加的训练成本。

**下一句预测** 下一句预测任务可以通过以下示例来说明。

输入 = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
标签 = IsNext（是下一句）

输入 = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
标签 = NotNext（不是下一句）

![image-20250419105019133](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419105019133.png)

**图3：预训练模型架构的差异。**BERT使用双向Transformer。OpenAI GPT使用从左到右的Transformer。ELMo使用独立训练的从左到右和从右到左的LSTM的连接来为下游任务生成特征。在这三者中，只有BERT的表示在所有层中同时以左右上下文为条件。除了架构差异外，BERT和OpenAI GPT是微调方法，而ELMo是基于特征的方法。

#### A.2 预训练程序

为了生成每个训练输入序列，我们从语料库中采样两个文本片段，我们称之为"句子"，尽管它们通常比单个句子长得多（但也可以更短）。第一个句子接收A嵌入，第二个接收B嵌入。50%的情况下，B是实际上跟随A的下一个句子，50%的情况下它是一个随机句子，这是为了"下一句预测"任务而做的。采样时确保组合长度≤512个标记。LM掩码在WordPiece分词后应用，使用15%的统一掩码率，且对部分词片段不给予特殊考虑。

我们使用256个序列的批量大小（256个序列 * 512个标记 = 128,000个标记/批次）训练了1,000,000步，这大约是对33亿单词语料库的40个轮次。我们使用Adam优化器，学习率为1e-4，β1 = 0.9，β2 = 0.999，L2权重衰减为0.01，前10,000步学习率预热，以及学习率的线性衰减。我们在所有层使用0.1的丢弃概率。我们使用gelu激活函数（Hendrycks和Gimpel，2016）而不是标准的relu，这一点与OpenAI GPT一致。训练损失是掩码LM似然的平均值和下一句预测似然的平均值之和。

BERT$_{BASE}$的训练是在Pod配置的4个Cloud TPU上进行的（总共16个TPU芯片）。BERT$_{LARGE}$的训练是在16个Cloud TPU上进行的（总共64个TPU芯片）。每次预训练需要4天完成。

较长的序列成本不成比例地高，因为注意力机制与序列长度成二次方关系。为了加速我们实验中的预训练，我们在90%的步骤中使用128的序列长度预训练模型。然后，我们训练剩余10%的步骤，使用512的序列长度来学习位置嵌入。

#### A.3 微调程序

对于微调，大多数模型超参数与预训练中的相同，但批量大小、学习率和训练轮数除外。丢弃概率始终保持为0.1。最优超参数值是针对具体任务的，但我们发现以下范围的可能值在所有任务中都能良好运行：

* 批量大小：16，32
* 学习率（Adam）：5e-5，3e-5，2e-5
* 轮数：2，3，4

我们还观察到，大型数据集（例如，10万+标记训练样本）对超参数选择的敏感度远低于小型数据集。微调通常非常快，因此合理的做法是对上述参数进行详尽搜索，并选择在开发集上表现最佳的模型。

#### A.4 BERT、ELMo和OpenAI GPT的比较

在这里，我们研究了最近流行的表示学习模型之间的差异，包括ELMo、OpenAI GPT和BERT。模型架构之间的比较如图3所示。请注意，除了架构差异外，BERT和OpenAI GPT是微调方法，而ELMo是基于特征的方法。

与BERT最具可比性的现有预训练方法是OpenAI GPT，它在大型文本语料库上训练了一个从左到右的Transformer语言模型。事实上，BERT中的许多设计决策都是有意使其尽可能接近GPT，以便两种方法可以进行最小化比较。本工作的核心论点是，第3.1节中介绍的双向性和两个预训练任务解释了大部分实证改进，但我们也注意到BERT和GPT训练方式之间存在其他几个差异：

• GPT在BooksCorpus（8亿词）上训练；BERT在BooksCorpus（8亿词）和维基百科（25亿词）上训练。

• GPT使用句子分隔符（[SEP]）和分类器标记（[CLS]），这些只在微调时引入；BERT在预训练期间学习[SEP]、[CLS]和句子A/B嵌入。

• GPT训练了1M步，批量大小为32,000词；BERT训练了1M步，批量大小为128,000词。

• GPT对所有微调实验使用相同的学习率5e-5；BERT选择在开发集上表现最佳的特定任务微调学习率。

为了隔离这些差异的影响，我们在第5.1节中进行了消融实验，证明大部分改进实际上来自于两个预训练任务和它们所实现的双向性。

#### A.5 不同任务上微调的图示

BERT在不同任务上微调的图示可以在图4中看到。我们的特定任务模型是通过将BERT与一个额外的输出层结合形成的，因此需要从头开始学习的参数数量最小。在这些任务中，(a)和(b)是序列级任务，而(c)和(d)是标记级任务。在图中，E表示输入嵌入，Ti表示标记i的上下文表示，[CLS]是用于分类输出的特殊符号，[SEP]是用于分隔非连续标记序列的特殊符号。

### B 详细实验设置

#### B.1 GLUE基准测试实验的详细描述

GLUE基准包括以下数据集，这些描述最初由Wang等人(2018a)总结：

MNLI 多类型自然语言推理是一项大规模的、众包的蕴含分类任务（Williams等人，2018）。给定一对句子，目标是预测第二个句子相对于第一个句子是蕴含、矛盾还是中性的。

QQP Quora问题对是一个二元分类任务，目标是确定在Quora上提出的两个问题在语义上是否等效（Chen等人，2018）。

QNLI 问题自然语言推理是斯坦福问答数据集（Rajpurkar等人，2016）的一个版本，已被转换为二元分类任务（Wang等人，2018a）。正例是包含正确答案的（问题，句子）对，负例是来自同一段落但不包含答案的（问题，句子）对。

SST-2 斯坦福情感树库是一个二元单句分类任务，由电影评论中提取的句子组成，并带有人类对其情感的标注（Socher等人，2013）。

CoLA 语言可接受性语料库是一个二元单句分类任务，其目标是预测一个英语句子在语言上是否"可接受"（Warstadt等人，2018）。

STS-B 语义文本相似度基准是从新闻标题和其他来源中抽取的句子对的集合（Cer等人，2017）。它们被标注了从1到5的分数，表示两个句子在语义上的相似程度。

MRPC 微软研究释义语料库由从在线新闻源自动提取的句子对组成，并带有人类对句子对是否在语义上等效的标注（Dolan和Brockett，2005）。

RTE 识别文本蕴含是一个类似于MNLI的二元蕴含任务，但训练数据要少得多（Bentivogli等人，2009）。

WNLI Winograd NLI是一个小型自然语言推理数据集（Levesque等人，2011）。GLUE网页指出这个数据集的构建存在问题，提交给GLUE的每个训练系统的表现都比预测多数类的65.1基线准确率差。因此，我们排除这个集合以对OpenAI GPT公平。对于我们的GLUE提交，我们始终预测多数类。

![image-20250419110220986](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419110220986.png)

**图4：在不同任务上微调BERT的图示。**

### C 额外的消融研究

#### C.1 训练步数的影响

图5展示了从预训练k步的检查点微调后的MNLI开发集准确率。这使我们能够回答以下问题：

1. 问题：BERT真的需要如此大量的预训练（128,000词/批次 * 1,000,000步）才能达到高微调准确率吗？
   回答：是的，与训练500k步相比，BERTBASE在训练1M步时在MNLI上获得了接近1.0%的额外准确率。

2. 问题：MLM预训练是否比LTR预训练收敛得更慢，因为每批次中只预测15%的词而不是每个词？
   回答：MLM模型确实比LTR模型收敛稍慢。然而，就绝对准确率而言，MLM模型几乎立即开始优于LTR模型。

![image-20250419110602618](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419110602618.png)

**图5：训练步数的消融实验。**这显示了微调后的MNLI准确率，从已经预训练k步的模型参数开始。x轴是k的值。

#### C.2 不同掩码程序的消融实验

在3.1节中，我们提到BERT在使用掩码语言模型（MLM）目标进行预训练时，对目标标记使用混合掩码策略。以下是评估不同掩码策略效果的消融研究。

请注意，掩码策略的目的是减少预训练和微调之间的不匹配，因为[MASK]符号在微调阶段从不出现。我们报告了MNLI和NER的开发集结果。对于NER，我们同时报告了微调和基于特征的方法，因为我们预计对于基于特征的方法，不匹配会被放大，因为模型将没有机会调整表示。

结果在表8中呈现。在表中，MASK表示我们在MLM中将目标标记替换为[MASK]符号；SAME表示我们保持目标标记不变；RND表示我们将目标标记替换为另一个随机标记。

表的左侧部分表示MLM预训练期间使用的特定策略的概率（BERT使用80%、10%、10%）。论文的右侧部分表示开发集结果。对于基于特征的方法，我们连接BERT的最后4层作为特征，这在5.3节中被证明是最佳方法。

从表中可以看出，微调对不同的掩码策略出人意料地稳健。然而，正如预期的那样，将基于特征的方法应用于NER时，仅使用MASK策略是有问题的。有趣的是，仅使用RND策略的表现也比我们的策略差得多。

![image-20250419110651404](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20250419110651404.png)

**表8：不同掩码策略的消融实验。**

